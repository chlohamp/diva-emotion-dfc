{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f6eed06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 actual clip counts:\n",
      "Run 1: 313 clips (S01E02R01_clip0000 to S01E02R01_clip0312)\n",
      "Run 2: 250 clips (S01E02R02_clip0000 to S01E02R02_clip0249)\n",
      "Run 3: 362 clips (S01E02R03_clip0000 to S01E02R03_clip0361)\n",
      "Run 4: 354 clips (S01E02R04_clip0000 to S01E02R04_clip0353)\n",
      "Run 5: 295 clips (S01E02R05_clip0000 to S01E02R05_clip0294)\n",
      "Run 6: 218 clips (S01E02R06_clip0000 to S01E02R06_clip0217)\n",
      "Run 7: 294 clips (S01E02R07_clip0000 to S01E02R07_clip0293)\n",
      "Total: 2086 clips\n",
      "Expected: 2079\n",
      "Matches expectation: False\n"
     ]
    }
   ],
   "source": [
    "# Quick verification of Episode 2 clip counts\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ANNOT_DIR = Path(\"dset/annotations\")\n",
    "s01e02_files = [f for f in ANNOT_DIR.iterdir() if f.name.startswith(\"S01E02\") and f.name.endswith(\"_FC.csv\")]\n",
    "s01e02_files.sort()\n",
    "\n",
    "print(\"Episode 2 actual clip counts:\")\n",
    "total = 0\n",
    "for file_path in s01e02_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    run_num = int(file_path.name.split('R')[1].split('_')[0])\n",
    "    \n",
    "    # Check first and last clip indices\n",
    "    first_clip = df.iloc[0]['index']\n",
    "    last_clip = df.iloc[-1]['index']\n",
    "    clip_count = len(df)\n",
    "    \n",
    "    print(f\"Run {run_num}: {clip_count} clips ({first_clip} to {last_clip})\")\n",
    "    total += clip_count\n",
    "\n",
    "print(f\"Total: {total} clips\")\n",
    "print(f\"Expected: 2079\")\n",
    "print(f\"Matches expectation: {total == 2079}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae12a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for gaps in clip sequences:\n",
      "Run 1:\n",
      "  Actual clip numbers: 0-312 (count: 313)\n",
      "  Expected range: 0-312 (count: 313)\n",
      "  ✅ Sequence is complete and correct\n",
      "  If run 1 should have 312 clips (0000-0311), then clip 0312 is extra\n",
      "Run 2:\n",
      "  Actual clip numbers: 0-249 (count: 250)\n",
      "  Expected range: 0-249 (count: 250)\n",
      "  ✅ Sequence is complete and correct\n",
      "Run 3:\n",
      "  Actual clip numbers: 0-361 (count: 362)\n",
      "  Expected range: 0-361 (count: 362)\n",
      "  ✅ Sequence is complete and correct\n",
      "Run 4:\n",
      "  Actual clip numbers: 0-353 (count: 354)\n",
      "  Expected range: 0-353 (count: 354)\n",
      "  ✅ Sequence is complete and correct\n",
      "Run 5:\n",
      "  Actual clip numbers: 0-294 (count: 295)\n",
      "  Expected range: 0-294 (count: 295)\n",
      "  ✅ Sequence is complete and correct\n",
      "Run 6:\n",
      "  Actual clip numbers: 0-217 (count: 218)\n",
      "  Expected range: 0-217 (count: 218)\n",
      "  ✅ Sequence is complete and correct\n",
      "Run 7:\n",
      "  Actual clip numbers: 0-293 (count: 294)\n",
      "  Expected range: 0-293 (count: 294)\n",
      "  ✅ Sequence is complete and correct\n",
      "\n",
      "If each run should have one fewer clip:\n",
      "Corrected total: 2079\n",
      "Target: 2079\n",
      "Matches target: True\n"
     ]
    }
   ],
   "source": [
    "# Check for missing clip indices that might explain the discrepancy\n",
    "print(\"Checking for gaps in clip sequences:\")\n",
    "for file_path in s01e02_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    run_num = int(file_path.name.split('R')[1].split('_')[0])\n",
    "    \n",
    "    # Extract clip numbers\n",
    "    clip_numbers = []\n",
    "    for index_val in df['index']:\n",
    "        if '_clip' in str(index_val):\n",
    "            clip_num = int(str(index_val).split('_clip')[-1])\n",
    "            clip_numbers.append(clip_num)\n",
    "    \n",
    "    clip_numbers.sort()\n",
    "    expected_range = list(range(0, len(clip_numbers)))\n",
    "    \n",
    "    print(f\"Run {run_num}:\")\n",
    "    print(f\"  Actual clip numbers: {clip_numbers[0]}-{clip_numbers[-1]} (count: {len(clip_numbers)})\")\n",
    "    print(f\"  Expected range: 0-{len(clip_numbers)-1} (count: {len(expected_range)})\")\n",
    "    \n",
    "    # Check for gaps\n",
    "    missing = set(expected_range) - set(clip_numbers)\n",
    "    extra = set(clip_numbers) - set(expected_range)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"  Missing clips: {sorted(missing)}\")\n",
    "    if extra:\n",
    "        print(f\"  Extra clips: {sorted(extra)}\")\n",
    "    if not missing and not extra and clip_numbers == expected_range:\n",
    "        print(f\"  ✅ Sequence is complete and correct\")\n",
    "    \n",
    "    # If expecting 312 clips but have 313, maybe clip should end at 0311?\n",
    "    if run_num == 1:\n",
    "        print(f\"  If run 1 should have 312 clips (0000-0311), then clip 0312 is extra\")\n",
    "        \n",
    "print(f\"\\nIf each run should have one fewer clip:\")\n",
    "corrected_total = 0\n",
    "for file_path in s01e02_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    corrected_count = len(df) - 1  # Remove one clip\n",
    "    corrected_total += corrected_count\n",
    "\n",
    "print(f\"Corrected total: {corrected_total}\")\n",
    "print(f\"Target: 2079\")\n",
    "print(f\"Matches target: {corrected_total == 2079}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3effd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca807dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ANNOT_DIR = Path(\"dset/annotations\")\n",
    "OUT_DIR = Path(\"dset/derivatives/annotations\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bd745e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_annotation_files(annotation_dir: Path):\n",
    "    pattern = re.compile(r\"^(S\\d+E\\d+R\\d+)_([A-Za-z]{2})\\.csv$\")\n",
    "    groups = {}\n",
    "    for p in sorted(annotation_dir.iterdir()):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        m = pattern.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        prefix = m.group(1)\n",
    "        annot = m.group(2)\n",
    "        groups.setdefault(prefix, []).append((annot, p))\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02935427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_columns(df: pd.DataFrame):\n",
    "    # detect index column\n",
    "    if \"index\" in df.columns:\n",
    "        idx_col = \"index\"\n",
    "    else:\n",
    "        # fallback: first column\n",
    "        idx_col = df.columns[0]\n",
    "\n",
    "    # detect valence and arousal columns (case-insensitive)\n",
    "    val_cols = [c for c in df.columns if \"valence\" in c.lower()]\n",
    "    aro_cols = [c for c in df.columns if \"arousal\" in c.lower()]\n",
    "\n",
    "    val_col = val_cols[0] if val_cols else None\n",
    "    aro_col = aro_cols[0] if aro_cols else None\n",
    "    return idx_col, val_col, aro_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eae54c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_group(prefix, files):\n",
    "    # files: list of (annotator_code, Path)\n",
    "    \n",
    "    # Skip if less than 2 files\n",
    "    if len(files) < 2:\n",
    "        print(f\"Skipping {prefix}: need at least 2 files, found {len(files)}\")\n",
    "        return None\n",
    "    \n",
    "    dfs = []\n",
    "    renamed = []\n",
    "    valid_count = 0  # Track valid files separately from loop index\n",
    "    \n",
    "    for annot, path in sorted(files):\n",
    "        df = pd.read_csv(path)\n",
    "        idx_col, val_col, aro_col = detect_columns(df)\n",
    "        if val_col is None or aro_col is None:\n",
    "            print(f\"Skipping {path.name}: could not find valence/arousal columns\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        # create minimal df using valid_count for consistent numbering\n",
    "        valid_count += 1\n",
    "        out = pd.DataFrame()\n",
    "        out[\"index\"] = df[idx_col]\n",
    "        out[f\"valence_{valid_count}\"] = df[val_col].values\n",
    "        out[f\"arousal_{valid_count}\"] = df[aro_col].values\n",
    "        dfs.append(out)\n",
    "        renamed.append((annot, f\"valence_{valid_count}\", f\"arousal_{valid_count}\"))\n",
    "\n",
    "    if len(dfs) < 2:\n",
    "        print(f\"Skipping {prefix}: need at least 2 valid files after filtering, found {len(dfs)}\")\n",
    "        return None\n",
    "\n",
    "    # merge on index\n",
    "    merged = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        merged = pd.merge(merged, d, on=\"index\", how=\"outer\")\n",
    "\n",
    "    # sort by index if numeric\n",
    "    try:\n",
    "        merged[\"index\"] = pd.to_numeric(merged[\"index\"])\n",
    "        merged = merged.sort_values(\"index\").reset_index(drop=True)\n",
    "    except Exception:\n",
    "        merged = merged.reset_index(drop=True)\n",
    "\n",
    "    # Reorder columns: index, all valence_*, then all arousal_*\n",
    "    val_cols = [c for c in merged.columns if c.startswith(\"valence_\")]\n",
    "    aro_cols = [c for c in merged.columns if c.startswith(\"arousal_\")]\n",
    "\n",
    "    def _num_suffix(colname):\n",
    "        parts = colname.split(\"_\")\n",
    "        try:\n",
    "            return int(parts[-1])\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    val_cols = sorted(val_cols, key=_num_suffix)\n",
    "    aro_cols = sorted(aro_cols, key=_num_suffix)\n",
    "\n",
    "    new_cols = [\"index\"] + val_cols + aro_cols\n",
    "    # keep any unexpected columns at the end (shouldn't typically occur)\n",
    "    tail = [c for c in merged.columns if c not in new_cols]\n",
    "    merged = merged[new_cols + tail]\n",
    "\n",
    "    # Remove rows with insufficient ratings and document removals\n",
    "    initial_rows = len(merged)\n",
    "    removed_clips = []\n",
    "    \n",
    "    # Extract episode and run info from prefix for sequential numbering\n",
    "    episode_match = re.match(r\"(S\\d+E\\d+)R(\\d+)\", prefix)\n",
    "    episode_id = episode_match.group(1) if episode_match else \"Unknown\"\n",
    "    run_number = int(episode_match.group(2)) if episode_match else 0\n",
    "    \n",
    "    # Check each row for sufficient ratings and valid scores\n",
    "    rows_to_keep = []\n",
    "    for i, row in merged.iterrows():\n",
    "        # Extract clip number from index (assuming format like \"S01E01R01_clip0001\")\n",
    "        try:\n",
    "            if isinstance(row['index'], str) and '_clip' in row['index']:\n",
    "                clip_num_str = row['index'].split('_clip')[-1]\n",
    "                clip_number = int(clip_num_str)\n",
    "            else:\n",
    "                # Fallback: use row position\n",
    "                clip_number = i + 1\n",
    "        except (ValueError, IndexError):\n",
    "            clip_number = i + 1\n",
    "        \n",
    "        # Count non-null valence ratings and check for scores > 7\n",
    "        val_ratings = []\n",
    "        val_invalid_scores = []\n",
    "        for col in val_cols:\n",
    "            if pd.notna(row[col]):\n",
    "                try:\n",
    "                    score = float(row[col])  # Convert to numeric\n",
    "                    if score > 7:\n",
    "                        val_invalid_scores.append(score)\n",
    "                    else:\n",
    "                        val_ratings.append(score)\n",
    "                except (ValueError, TypeError):\n",
    "                    # Skip non-numeric values\n",
    "                    print(f\"Warning: Non-numeric valence value {row[col]} in {col} for index {row['index']}\")\n",
    "                    continue\n",
    "        \n",
    "        # Count non-null arousal ratings and check for scores > 7\n",
    "        aro_ratings = []\n",
    "        aro_invalid_scores = []\n",
    "        for col in aro_cols:\n",
    "            if pd.notna(row[col]):\n",
    "                try:\n",
    "                    score = float(row[col])  # Convert to numeric\n",
    "                    if score > 7:\n",
    "                        aro_invalid_scores.append(score)\n",
    "                    else:\n",
    "                        aro_ratings.append(score)\n",
    "                except (ValueError, TypeError):\n",
    "                    # Skip non-numeric values\n",
    "                    print(f\"Warning: Non-numeric arousal value {row[col]} in {col} for index {row['index']}\")\n",
    "                    continue\n",
    "        \n",
    "        # Determine removal reason\n",
    "        removal_reasons = []\n",
    "        if len(val_ratings) < 2:\n",
    "            removal_reasons.append(f\"insufficient valence ratings ({len(val_ratings)})\")\n",
    "        if len(aro_ratings) < 2:\n",
    "            removal_reasons.append(f\"insufficient arousal ratings ({len(aro_ratings)})\")\n",
    "        if val_invalid_scores:\n",
    "            removal_reasons.append(f\"valence scores > 7: {val_invalid_scores}\")\n",
    "        if aro_invalid_scores:\n",
    "            removal_reasons.append(f\"arousal scores > 7: {aro_invalid_scores}\")\n",
    "        \n",
    "        # Keep row only if it has at least 2 valid ratings for both valence and arousal\n",
    "        if len(val_ratings) >= 2 and len(aro_ratings) >= 2 and not val_invalid_scores and not aro_invalid_scores:\n",
    "            rows_to_keep.append(i)\n",
    "        else:\n",
    "            removed_clips.append({\n",
    "                'episode': episode_id,\n",
    "                'run': run_number,\n",
    "                'run_prefix': prefix,\n",
    "                'index': row['index'],\n",
    "                'clip_number': clip_number,\n",
    "                'valence_ratings': len(val_ratings),\n",
    "                'arousal_ratings': len(aro_ratings),\n",
    "                'valence_invalid_scores': len(val_invalid_scores),\n",
    "                'arousal_invalid_scores': len(aro_invalid_scores),\n",
    "                'reason': \"; \".join(removal_reasons)\n",
    "            })\n",
    "    \n",
    "    # Filter the dataframe\n",
    "    if rows_to_keep:\n",
    "        merged_filtered = merged.iloc[rows_to_keep].reset_index(drop=True)\n",
    "    else:\n",
    "        merged_filtered = pd.DataFrame(columns=merged.columns)\n",
    "    \n",
    "    final_rows = len(merged_filtered)\n",
    "    \n",
    "    # Report removals\n",
    "    if removed_clips:\n",
    "        print(f\"  Removed {len(removed_clips)}/{initial_rows} clips with insufficient ratings:\")\n",
    "        for clip in removed_clips[:5]:  # Show first 5 removals\n",
    "            print(f\"    - {clip['index']}: {clip['reason']}\")\n",
    "        if len(removed_clips) > 5:\n",
    "            print(f\"    ... and {len(removed_clips) - 5} more\")\n",
    "    \n",
    "    return merged_filtered, renamed, removed_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f8ec108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 36/334 clips with insufficient ratings:\n",
      "    - S01E01R01_clip0029: insufficient arousal ratings (1); arousal scores > 7: [10.0]\n",
      "    - S01E01R01_clip0030: insufficient arousal ratings (1); arousal scores > 7: [10.0]\n",
      "    - S01E01R01_clip0031: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E01R01_clip0032: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E01R01_clip0033: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    ... and 31 more\n",
      "Processed S01E01R01 (2 files -> 298 rows, 36 clips removed)\n",
      "Processed S01E01R02 (2 files -> 276 rows, 0 clips removed)\n",
      "  Removed 48/275 clips with insufficient ratings:\n",
      "    - S01E01R03_clip0147: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E01R03_clip0148: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E01R03_clip0149: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E01R03_clip0150: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E01R03_clip0151: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 43 more\n",
      "Processed S01E01R03 (2 files -> 227 rows, 48 clips removed)\n",
      "Skipping S01E01R04: need at least 2 files, found 1\n",
      "No valid files for S01E01R04\n",
      "Skipping S01E01R05: need at least 2 files, found 1\n",
      "No valid files for S01E01R05\n",
      "Skipping S01E01R06: need at least 2 files, found 1\n",
      "No valid files for S01E01R06\n",
      "  Removed 10/313 clips with insufficient ratings:\n",
      "    - S01E02R01_clip0000: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "    - S01E02R01_clip0014: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R01_clip0031: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E02R01_clip0178: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "    - S01E02R01_clip0179: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "    ... and 5 more\n",
      "Processed S01E02R01 (2 files -> 303 rows, 10 clips removed)\n",
      "  Removed 20/250 clips with insufficient ratings:\n",
      "    - S01E02R02_clip0015: arousal scores > 7: [9.0]\n",
      "    - S01E02R02_clip0016: arousal scores > 7: [9.0]\n",
      "    - S01E02R02_clip0026: valence scores > 7: [9.0]\n",
      "    - S01E02R02_clip0034: valence scores > 7: [8.0]\n",
      "    - S01E02R02_clip0035: valence scores > 7: [8.0]\n",
      "    ... and 15 more\n",
      "Processed S01E02R02 (8 files -> 230 rows, 20 clips removed)\n",
      "  Removed 6/362 clips with insufficient ratings:\n",
      "    - S01E02R03_clip0058: insufficient valence ratings (1); valence scores > 7: [9.0]\n",
      "    - S01E02R03_clip0059: insufficient valence ratings (1); valence scores > 7: [9.0]\n",
      "    - S01E02R03_clip0175: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R03_clip0176: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R03_clip0177: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 1 more\n",
      "Processed S01E02R03 (2 files -> 356 rows, 6 clips removed)\n",
      "  Removed 4/354 clips with insufficient ratings:\n",
      "    - S01E02R04_clip0162: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R04_clip0219: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R04_clip0224: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R04_clip0226: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "Processed S01E02R04 (2 files -> 350 rows, 4 clips removed)\n",
      "  Removed 3/295 clips with insufficient ratings:\n",
      "    - S01E02R05_clip0048: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E02R05_clip0051: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R05_clip0052: insufficient arousal ratings (1); arousal scores > 7: [10.0]\n",
      "Processed S01E02R05 (2 files -> 292 rows, 3 clips removed)\n",
      "  Removed 26/218 clips with insufficient ratings:\n",
      "    - S01E02R06_clip0082: insufficient arousal ratings (1)\n",
      "    - S01E02R06_clip0083: insufficient arousal ratings (1)\n",
      "    - S01E02R06_clip0084: insufficient arousal ratings (1)\n",
      "    - S01E02R06_clip0085: insufficient arousal ratings (1)\n",
      "    - S01E02R06_clip0086: insufficient arousal ratings (1)\n",
      "    ... and 21 more\n",
      "Processed S01E02R06 (2 files -> 192 rows, 26 clips removed)\n",
      "  Removed 13/294 clips with insufficient ratings:\n",
      "    - S01E02R07_clip0027: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R07_clip0033: insufficient arousal ratings (0); arousal scores > 7: [9.0, 9.0]\n",
      "    - S01E02R07_clip0068: insufficient arousal ratings (0); arousal scores > 7: [9.0, 8.0]\n",
      "    - S01E02R07_clip0102: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E02R07_clip0103: insufficient arousal ratings (0); arousal scores > 7: [9.0, 8.0]\n",
      "    ... and 8 more\n",
      "Processed S01E02R07 (2 files -> 281 rows, 13 clips removed)\n",
      "  Removed 11/272 clips with insufficient ratings:\n",
      "    - S01E03R01_clip0021: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E03R01_clip0022: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E03R01_clip0034: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E03R01_clip0051: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E03R01_clip0055: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    ... and 6 more\n",
      "Processed S01E03R01 (2 files -> 261 rows, 11 clips removed)\n",
      "  Removed 2/311 clips with insufficient ratings:\n",
      "    - S01E03R02_clip0307: insufficient arousal ratings (1)\n",
      "    - S01E03R02_clip0310: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "Processed S01E03R02 (2 files -> 309 rows, 2 clips removed)\n",
      "Warning: Non-numeric valence value \\ in valence_1 for index S01E03R03_clip0137\n",
      "  Removed 1/317 clips with insufficient ratings:\n",
      "    - S01E03R03_clip0137: insufficient valence ratings (1)\n",
      "Processed S01E03R03 (2 files -> 316 rows, 1 clips removed)\n",
      "Processed S01E03R04 (2 files -> 359 rows, 0 clips removed)\n",
      "  Removed 2/311 clips with insufficient ratings:\n",
      "    - S01E03R02_clip0307: insufficient arousal ratings (1)\n",
      "    - S01E03R02_clip0310: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "Processed S01E03R02 (2 files -> 309 rows, 2 clips removed)\n",
      "Warning: Non-numeric valence value \\ in valence_1 for index S01E03R03_clip0137\n",
      "  Removed 1/317 clips with insufficient ratings:\n",
      "    - S01E03R03_clip0137: insufficient valence ratings (1)\n",
      "Processed S01E03R03 (2 files -> 316 rows, 1 clips removed)\n",
      "Processed S01E03R04 (2 files -> 359 rows, 0 clips removed)\n",
      "Processed S01E03R05 (2 files -> 268 rows, 0 clips removed)\n",
      "  Removed 8/416 clips with insufficient ratings:\n",
      "    - S01E03R06_clip0276: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0278: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0279: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0280: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0281: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 3 more\n",
      "Processed S01E03R06 (2 files -> 408 rows, 8 clips removed)\n",
      "  Removed 1/308 clips with insufficient ratings:\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "Processed S01E04R01 (2 files -> 307 rows, 1 clips removed)\n",
      "Processed S01E04R02 (2 files -> 296 rows, 0 clips removed)\n",
      "  Removed 3/296 clips with insufficient ratings:\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "Processed S01E04R03 (2 files -> 293 rows, 3 clips removed)\n",
      "  Removed 16/334 clips with insufficient ratings:\n",
      "    - S01E04R04_clip0219: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0220: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0221: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0222: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0223: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 11 more\n",
      "Processed S01E04R04 (2 files -> 318 rows, 16 clips removed)\n",
      "Processed S01E04R05 (2 files -> 230 rows, 0 clips removed)\n",
      "\n",
      "Creating episode-level files for 4 episodes...\n",
      "\n",
      "Combining runs for S01E01:\n",
      "  Added S01E01R01: 298 clips (2 files, 36 clips removed)\n",
      "  Added S01E01R02: 276 clips (2 files, 0 clips removed)\n",
      "  Added S01E01R03: 227 clips (2 files, 48 clips removed)\n",
      "  Saved dset/derivatives/annotations/S01E01.csv: 801 total clips across 3 runs\n",
      "\n",
      "Combining runs for S01E02:\n",
      "  Added S01E02R01: 303 clips (2 files, 10 clips removed)\n",
      "  Added S01E02R02: 230 clips (8 files, 20 clips removed)\n",
      "  Added S01E02R03: 356 clips (2 files, 6 clips removed)\n",
      "  Added S01E02R04: 350 clips (2 files, 4 clips removed)\n",
      "  Added S01E02R05: 292 clips (2 files, 3 clips removed)\n",
      "  Added S01E02R06: 192 clips (2 files, 26 clips removed)\n",
      "  Added S01E02R07: 281 clips (2 files, 13 clips removed)\n",
      "Processed S01E03R05 (2 files -> 268 rows, 0 clips removed)\n",
      "  Removed 8/416 clips with insufficient ratings:\n",
      "    - S01E03R06_clip0276: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0278: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0279: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0280: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0281: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 3 more\n",
      "Processed S01E03R06 (2 files -> 408 rows, 8 clips removed)\n",
      "  Removed 1/308 clips with insufficient ratings:\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "Processed S01E04R01 (2 files -> 307 rows, 1 clips removed)\n",
      "Processed S01E04R02 (2 files -> 296 rows, 0 clips removed)\n",
      "  Removed 3/296 clips with insufficient ratings:\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "Processed S01E04R03 (2 files -> 293 rows, 3 clips removed)\n",
      "  Removed 16/334 clips with insufficient ratings:\n",
      "    - S01E04R04_clip0219: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0220: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0221: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0222: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0223: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 11 more\n",
      "Processed S01E04R04 (2 files -> 318 rows, 16 clips removed)\n",
      "Processed S01E04R05 (2 files -> 230 rows, 0 clips removed)\n",
      "\n",
      "Creating episode-level files for 4 episodes...\n",
      "\n",
      "Combining runs for S01E01:\n",
      "  Added S01E01R01: 298 clips (2 files, 36 clips removed)\n",
      "  Added S01E01R02: 276 clips (2 files, 0 clips removed)\n",
      "  Added S01E01R03: 227 clips (2 files, 48 clips removed)\n",
      "  Saved dset/derivatives/annotations/S01E01.csv: 801 total clips across 3 runs\n",
      "\n",
      "Combining runs for S01E02:\n",
      "  Added S01E02R01: 303 clips (2 files, 10 clips removed)\n",
      "  Added S01E02R02: 230 clips (8 files, 20 clips removed)\n",
      "  Added S01E02R03: 356 clips (2 files, 6 clips removed)\n",
      "  Added S01E02R04: 350 clips (2 files, 4 clips removed)\n",
      "  Added S01E02R05: 292 clips (2 files, 3 clips removed)\n",
      "  Added S01E02R06: 192 clips (2 files, 26 clips removed)\n",
      "  Added S01E02R07: 281 clips (2 files, 13 clips removed)\n",
      "  Saved dset/derivatives/annotations/S01E02.csv: 2004 total clips across 7 runs\n",
      "\n",
      "Combining runs for S01E03:\n",
      "  Added S01E03R01: 261 clips (2 files, 11 clips removed)\n",
      "  Added S01E03R02: 309 clips (2 files, 2 clips removed)\n",
      "  Added S01E03R03: 316 clips (2 files, 1 clips removed)\n",
      "  Added S01E03R04: 359 clips (2 files, 0 clips removed)\n",
      "  Added S01E03R05: 268 clips (2 files, 0 clips removed)\n",
      "  Added S01E03R06: 408 clips (2 files, 8 clips removed)\n",
      "  Saved dset/derivatives/annotations/S01E03.csv: 1921 total clips across 6 runs\n",
      "\n",
      "Combining runs for S01E04:\n",
      "  Added S01E04R01: 307 clips (2 files, 1 clips removed)\n",
      "  Added S01E04R02: 296 clips (2 files, 0 clips removed)\n",
      "  Added S01E04R03: 293 clips (2 files, 3 clips removed)\n",
      "  Added S01E04R04: 318 clips (2 files, 16 clips removed)\n",
      "  Added S01E04R05: 230 clips (2 files, 0 clips removed)\n",
      "  Saved dset/derivatives/annotations/S01E04.csv: 1444 total clips across 5 runs\n",
      "\n",
      "Episode-level files created:\n",
      "  S01E01: 3 runs, 801 clips -> dset/derivatives/annotations/S01E01.csv\n",
      "  S01E02: 7 runs, 2004 clips -> dset/derivatives/annotations/S01E02.csv\n",
      "  S01E03: 6 runs, 1921 clips -> dset/derivatives/annotations/S01E03.csv\n",
      "  S01E04: 5 runs, 1444 clips -> dset/derivatives/annotations/S01E04.csv\n",
      "\n",
      "Total clips removed across all episodes: 208\n",
      "\n",
      "DEBUG: Removal dataframe columns: ['episode', 'run', 'run_prefix', 'index', 'clip_number', 'valence_ratings', 'arousal_ratings', 'valence_invalid_scores', 'arousal_invalid_scores', 'reason']\n",
      "DEBUG: First few rows of removal data:\n",
      "  episode  run run_prefix               index  clip_number  valence_ratings  \\\n",
      "0  S01E01    1  S01E01R01  S01E01R01_clip0029           29                2   \n",
      "1  S01E01    1  S01E01R01  S01E01R01_clip0030           30                2   \n",
      "2  S01E01    1  S01E01R01  S01E01R01_clip0031           31                2   \n",
      "3  S01E01    1  S01E01R01  S01E01R01_clip0032           32                2   \n",
      "4  S01E01    1  S01E01R01  S01E01R01_clip0033           33                2   \n",
      "\n",
      "   arousal_ratings  valence_invalid_scores  arousal_invalid_scores  \\\n",
      "0                1                       0                       1   \n",
      "1                1                       0                       1   \n",
      "2                1                       0                       1   \n",
      "3                1                       0                       1   \n",
      "4                1                       0                       1   \n",
      "\n",
      "                                              reason  \n",
      "0  insufficient arousal ratings (1); arousal scor...  \n",
      "1  insufficient arousal ratings (1); arousal scor...  \n",
      "2  insufficient arousal ratings (1); arousal scor...  \n",
      "3  insufficient arousal ratings (1); arousal scor...  \n",
      "4  insufficient arousal ratings (1); arousal scor...  \n",
      "\n",
      "Processing episode positions for S01E01...\n",
      "  Found prefixes for S01E01: ['S01E01R01', 'S01E01R02', 'S01E01R03', 'S01E01R04', 'S01E01R05', 'S01E01R06']\n",
      "    S01E01R01: Processing 334 clips using NC file, starting at position 1\n",
      "    S01E01R01: Clips S01E01R01_clip0000 to S01E01R01_clip0333 -> positions 1 to 334\n",
      "    S01E01R02: Processing 276 clips using NC file, starting at position 335\n",
      "    S01E01R02: Clips S01E01R02_clip0000 to S01E01R02_clip0275 -> positions 335 to 610\n",
      "    S01E01R03: Processing 275 clips using NC file, starting at position 611\n",
      "    S01E01R03: Clips S01E01R03_clip0000 to S01E01R03_clip0274 -> positions 611 to 885\n",
      "    S01E01R04: Processing 286 clips using NC file, starting at position 886\n",
      "    S01E01R04: Clips S01E01R04_clip0000 to S01E01R04_clip0285 -> positions 886 to 1171\n",
      "    S01E01R05: Processing 312 clips using NC file, starting at position 1172\n",
      "    S01E01R05: Clips S01E01R05_clip0000 to S01E01R05_clip0311 -> positions 1172 to 1483\n",
      "    S01E01R06: Processing 333 clips using NC file, starting at position 1484\n",
      "    S01E01R06: Clips S01E01R06_clip0000 to S01E01R06_clip0332 -> positions 1484 to 1816\n",
      "  Total clips for S01E01: 1816\n",
      "  Episode positions range: 1 to 1816\n",
      "  Sample position mapping: [('S01E01R01_clip0000', 1), ('S01E01R01_clip0001', 2), ('S01E01R01_clip0002', 3)]\n",
      "  Mapped 84/84 positions for S01E01\n",
      "\n",
      "Processing episode positions for S01E02...\n",
      "  Found prefixes for S01E02: ['S01E02R01', 'S01E02R02', 'S01E02R03', 'S01E02R04', 'S01E02R05', 'S01E02R06', 'S01E02R07']\n",
      "    S01E02R01: Processing 313 clips using FC file, starting at position 1\n",
      "    S01E02R01: Clips S01E02R01_clip0000 to S01E02R01_clip0312 -> positions 1 to 313\n",
      "    S01E02R02: Processing 250 clips using FC file, starting at position 314\n",
      "    S01E02R02: Clips S01E02R02_clip0000 to S01E02R02_clip0249 -> positions 314 to 563\n",
      "    S01E02R03: Processing 362 clips using FC file, starting at position 564\n",
      "    S01E02R03: Clips S01E02R03_clip0000 to S01E02R03_clip0361 -> positions 564 to 925\n",
      "    S01E02R04: Processing 354 clips using FC file, starting at position 926\n",
      "    S01E02R04: Clips S01E02R04_clip0000 to S01E02R04_clip0353 -> positions 926 to 1279\n",
      "    S01E02R05: Processing 295 clips using FC file, starting at position 1280\n",
      "    S01E02R05: Clips S01E02R05_clip0000 to S01E02R05_clip0294 -> positions 1280 to 1574\n",
      "    S01E02R06: Processing 218 clips using FC file, starting at position 1575\n",
      "    S01E02R06: Clips S01E02R06_clip0000 to S01E02R06_clip0217 -> positions 1575 to 1792\n",
      "    S01E02R07: Processing 294 clips using FC file, starting at position 1793\n",
      "    S01E02R07: Clips S01E02R07_clip0000 to S01E02R07_clip0293 -> positions 1793 to 2086\n",
      "  Total clips for S01E02: 2086\n",
      "  Episode positions range: 1 to 2086\n",
      "  Sample position mapping: [('S01E02R01_clip0000', 1), ('S01E02R01_clip0001', 2), ('S01E02R01_clip0002', 3)]\n",
      "  Mapped 82/82 positions for S01E02\n",
      "\n",
      "Processing episode positions for S01E03...\n",
      "  Found prefixes for S01E03: ['S01E03R01', 'S01E03R02', 'S01E03R03', 'S01E03R04', 'S01E03R05', 'S01E03R06']\n",
      "    S01E03R01: Processing 272 clips using FC file, starting at position 1\n",
      "    S01E03R01: Clips S01E03R01_clip0000 to S01E03R01_clip0271 -> positions 1 to 272\n",
      "    S01E03R02: Processing 311 clips using FC file, starting at position 273\n",
      "    S01E03R02: Clips S01E03R02_clip0000 to S01E03R02_clip0310 -> positions 273 to 583\n",
      "    S01E03R03: Processing 317 clips using FC file, starting at position 584\n",
      "    S01E03R03: Clips S01E03R03_clip0000 to S01E03R03_clip0316 -> positions 584 to 900\n",
      "    S01E03R04: Processing 359 clips using FC file, starting at position 901\n",
      "    S01E03R04: Clips S01E03R04_clip0000 to S01E03R04_clip0358 -> positions 901 to 1259\n",
      "    S01E03R05: Processing 268 clips using FC file, starting at position 1260\n",
      "    S01E03R05: Clips S01E03R05_clip0000 to S01E03R05_clip0267 -> positions 1260 to 1527\n",
      "    S01E03R06: Processing 416 clips using FC file, starting at position 1528\n",
      "    S01E03R06: Clips S01E03R06_clip0000 to S01E03R06_clip0415 -> positions 1528 to 1943\n",
      "  Total clips for S01E03: 1943\n",
      "  Episode positions range: 1 to 1943\n",
      "  Sample position mapping: [('S01E03R01_clip0000', 1), ('S01E03R01_clip0001', 2), ('S01E03R01_clip0002', 3)]\n",
      "  Mapped 22/22 positions for S01E03\n",
      "\n",
      "Processing episode positions for S01E04...\n",
      "  Found prefixes for S01E04: ['S01E04R01', 'S01E04R02', 'S01E04R03', 'S01E04R04', 'S01E04R05']\n",
      "    S01E04R01: Processing 307 clips using AT file, starting at position 1\n",
      "    S01E04R01: Clips S01E04R01_clip0000 to S01E04R01_clip0306 -> positions 1 to 307\n",
      "    S01E04R02: Processing 296 clips using AT file, starting at position 308\n",
      "    S01E04R02: Clips S01E04R02_clip0000 to S01E04R02_clip0295 -> positions 308 to 603\n",
      "    S01E04R03: Processing 293 clips using AT file, starting at position 604\n",
      "    S01E04R03: Clips S01E04R03_clip0000 to S01E04R03_clip0292 -> positions 604 to 896\n",
      "    S01E04R04: Processing 334 clips using AT file, starting at position 897\n",
      "    S01E04R04: Clips S01E04R04_clip0000 to S01E04R04_clip0333 -> positions 897 to 1230\n",
      "    S01E04R05: Processing 230 clips using AT file, starting at position 1231\n",
      "    S01E04R05: Clips S01E04R05_clip0000 to S01E04R05_clip0229 -> positions 1231 to 1460\n",
      "  Total clips for S01E04: 1460\n",
      "  Episode positions range: 1 to 1460\n",
      "  Sample position mapping: [('S01E04R01_clip0000', 1), ('S01E04R01_clip0001', 2), ('S01E04R01_clip0002', 3)]\n",
      "  Mapped 16/20 positions for S01E04\n",
      "  WARNING: 4 clips in S01E04 could not be mapped\n",
      "    Used clip_number 308 as fallback position for nan\n",
      "    Used clip_number 294 as fallback position for nan\n",
      "    Used clip_number 295 as fallback position for nan\n",
      "    Used clip_number 296 as fallback position for nan\n",
      "\n",
      "Saved removal log: dset/derivatives/annotations/removed_clips_log.csv\n",
      "Removal log contains episode_position column: True\n",
      "Episode positions mapped: 208/208 (100.0%)\n",
      "\n",
      "Sample episode positions:\n",
      "  S01E01:\n",
      "    S01E01R01_clip0029 (clip 29) -> episode position 30.0\n",
      "    S01E01R01_clip0030 (clip 30) -> episode position 31.0\n",
      "    S01E01R01_clip0031 (clip 31) -> episode position 32.0\n",
      "  S01E02:\n",
      "    S01E02R01_clip0000 (clip 0) -> episode position 1.0\n",
      "    S01E02R01_clip0014 (clip 14) -> episode position 15.0\n",
      "    S01E02R01_clip0031 (clip 31) -> episode position 32.0\n",
      "  S01E03:\n",
      "    S01E03R01_clip0021 (clip 21) -> episode position 22.0\n",
      "    S01E03R01_clip0022 (clip 22) -> episode position 23.0\n",
      "    S01E03R01_clip0034 (clip 34) -> episode position 35.0\n",
      "  S01E04:\n",
      "    nan (clip 308) -> episode position 308.0\n",
      "    nan (clip 294) -> episode position 294.0\n",
      "    nan (clip 295) -> episode position 295.0\n",
      "\n",
      "Removals by episode:\n",
      "  S01E01: 84 clips removed, 84 with positions\n",
      "  S01E02: 82 clips removed, 82 with positions\n",
      "  S01E03: 22 clips removed, 22 with positions\n",
      "  S01E04: 20 clips removed, 20 with positions\n",
      "  Saved dset/derivatives/annotations/S01E02.csv: 2004 total clips across 7 runs\n",
      "\n",
      "Combining runs for S01E03:\n",
      "  Added S01E03R01: 261 clips (2 files, 11 clips removed)\n",
      "  Added S01E03R02: 309 clips (2 files, 2 clips removed)\n",
      "  Added S01E03R03: 316 clips (2 files, 1 clips removed)\n",
      "  Added S01E03R04: 359 clips (2 files, 0 clips removed)\n",
      "  Added S01E03R05: 268 clips (2 files, 0 clips removed)\n",
      "  Added S01E03R06: 408 clips (2 files, 8 clips removed)\n",
      "  Saved dset/derivatives/annotations/S01E03.csv: 1921 total clips across 6 runs\n",
      "\n",
      "Combining runs for S01E04:\n",
      "  Added S01E04R01: 307 clips (2 files, 1 clips removed)\n",
      "  Added S01E04R02: 296 clips (2 files, 0 clips removed)\n",
      "  Added S01E04R03: 293 clips (2 files, 3 clips removed)\n",
      "  Added S01E04R04: 318 clips (2 files, 16 clips removed)\n",
      "  Added S01E04R05: 230 clips (2 files, 0 clips removed)\n",
      "  Saved dset/derivatives/annotations/S01E04.csv: 1444 total clips across 5 runs\n",
      "\n",
      "Episode-level files created:\n",
      "  S01E01: 3 runs, 801 clips -> dset/derivatives/annotations/S01E01.csv\n",
      "  S01E02: 7 runs, 2004 clips -> dset/derivatives/annotations/S01E02.csv\n",
      "  S01E03: 6 runs, 1921 clips -> dset/derivatives/annotations/S01E03.csv\n",
      "  S01E04: 5 runs, 1444 clips -> dset/derivatives/annotations/S01E04.csv\n",
      "\n",
      "Total clips removed across all episodes: 208\n",
      "\n",
      "DEBUG: Removal dataframe columns: ['episode', 'run', 'run_prefix', 'index', 'clip_number', 'valence_ratings', 'arousal_ratings', 'valence_invalid_scores', 'arousal_invalid_scores', 'reason']\n",
      "DEBUG: First few rows of removal data:\n",
      "  episode  run run_prefix               index  clip_number  valence_ratings  \\\n",
      "0  S01E01    1  S01E01R01  S01E01R01_clip0029           29                2   \n",
      "1  S01E01    1  S01E01R01  S01E01R01_clip0030           30                2   \n",
      "2  S01E01    1  S01E01R01  S01E01R01_clip0031           31                2   \n",
      "3  S01E01    1  S01E01R01  S01E01R01_clip0032           32                2   \n",
      "4  S01E01    1  S01E01R01  S01E01R01_clip0033           33                2   \n",
      "\n",
      "   arousal_ratings  valence_invalid_scores  arousal_invalid_scores  \\\n",
      "0                1                       0                       1   \n",
      "1                1                       0                       1   \n",
      "2                1                       0                       1   \n",
      "3                1                       0                       1   \n",
      "4                1                       0                       1   \n",
      "\n",
      "                                              reason  \n",
      "0  insufficient arousal ratings (1); arousal scor...  \n",
      "1  insufficient arousal ratings (1); arousal scor...  \n",
      "2  insufficient arousal ratings (1); arousal scor...  \n",
      "3  insufficient arousal ratings (1); arousal scor...  \n",
      "4  insufficient arousal ratings (1); arousal scor...  \n",
      "\n",
      "Processing episode positions for S01E01...\n",
      "  Found prefixes for S01E01: ['S01E01R01', 'S01E01R02', 'S01E01R03', 'S01E01R04', 'S01E01R05', 'S01E01R06']\n",
      "    S01E01R01: Processing 334 clips using NC file, starting at position 1\n",
      "    S01E01R01: Clips S01E01R01_clip0000 to S01E01R01_clip0333 -> positions 1 to 334\n",
      "    S01E01R02: Processing 276 clips using NC file, starting at position 335\n",
      "    S01E01R02: Clips S01E01R02_clip0000 to S01E01R02_clip0275 -> positions 335 to 610\n",
      "    S01E01R03: Processing 275 clips using NC file, starting at position 611\n",
      "    S01E01R03: Clips S01E01R03_clip0000 to S01E01R03_clip0274 -> positions 611 to 885\n",
      "    S01E01R04: Processing 286 clips using NC file, starting at position 886\n",
      "    S01E01R04: Clips S01E01R04_clip0000 to S01E01R04_clip0285 -> positions 886 to 1171\n",
      "    S01E01R05: Processing 312 clips using NC file, starting at position 1172\n",
      "    S01E01R05: Clips S01E01R05_clip0000 to S01E01R05_clip0311 -> positions 1172 to 1483\n",
      "    S01E01R06: Processing 333 clips using NC file, starting at position 1484\n",
      "    S01E01R06: Clips S01E01R06_clip0000 to S01E01R06_clip0332 -> positions 1484 to 1816\n",
      "  Total clips for S01E01: 1816\n",
      "  Episode positions range: 1 to 1816\n",
      "  Sample position mapping: [('S01E01R01_clip0000', 1), ('S01E01R01_clip0001', 2), ('S01E01R01_clip0002', 3)]\n",
      "  Mapped 84/84 positions for S01E01\n",
      "\n",
      "Processing episode positions for S01E02...\n",
      "  Found prefixes for S01E02: ['S01E02R01', 'S01E02R02', 'S01E02R03', 'S01E02R04', 'S01E02R05', 'S01E02R06', 'S01E02R07']\n",
      "    S01E02R01: Processing 313 clips using FC file, starting at position 1\n",
      "    S01E02R01: Clips S01E02R01_clip0000 to S01E02R01_clip0312 -> positions 1 to 313\n",
      "    S01E02R02: Processing 250 clips using FC file, starting at position 314\n",
      "    S01E02R02: Clips S01E02R02_clip0000 to S01E02R02_clip0249 -> positions 314 to 563\n",
      "    S01E02R03: Processing 362 clips using FC file, starting at position 564\n",
      "    S01E02R03: Clips S01E02R03_clip0000 to S01E02R03_clip0361 -> positions 564 to 925\n",
      "    S01E02R04: Processing 354 clips using FC file, starting at position 926\n",
      "    S01E02R04: Clips S01E02R04_clip0000 to S01E02R04_clip0353 -> positions 926 to 1279\n",
      "    S01E02R05: Processing 295 clips using FC file, starting at position 1280\n",
      "    S01E02R05: Clips S01E02R05_clip0000 to S01E02R05_clip0294 -> positions 1280 to 1574\n",
      "    S01E02R06: Processing 218 clips using FC file, starting at position 1575\n",
      "    S01E02R06: Clips S01E02R06_clip0000 to S01E02R06_clip0217 -> positions 1575 to 1792\n",
      "    S01E02R07: Processing 294 clips using FC file, starting at position 1793\n",
      "    S01E02R07: Clips S01E02R07_clip0000 to S01E02R07_clip0293 -> positions 1793 to 2086\n",
      "  Total clips for S01E02: 2086\n",
      "  Episode positions range: 1 to 2086\n",
      "  Sample position mapping: [('S01E02R01_clip0000', 1), ('S01E02R01_clip0001', 2), ('S01E02R01_clip0002', 3)]\n",
      "  Mapped 82/82 positions for S01E02\n",
      "\n",
      "Processing episode positions for S01E03...\n",
      "  Found prefixes for S01E03: ['S01E03R01', 'S01E03R02', 'S01E03R03', 'S01E03R04', 'S01E03R05', 'S01E03R06']\n",
      "    S01E03R01: Processing 272 clips using FC file, starting at position 1\n",
      "    S01E03R01: Clips S01E03R01_clip0000 to S01E03R01_clip0271 -> positions 1 to 272\n",
      "    S01E03R02: Processing 311 clips using FC file, starting at position 273\n",
      "    S01E03R02: Clips S01E03R02_clip0000 to S01E03R02_clip0310 -> positions 273 to 583\n",
      "    S01E03R03: Processing 317 clips using FC file, starting at position 584\n",
      "    S01E03R03: Clips S01E03R03_clip0000 to S01E03R03_clip0316 -> positions 584 to 900\n",
      "    S01E03R04: Processing 359 clips using FC file, starting at position 901\n",
      "    S01E03R04: Clips S01E03R04_clip0000 to S01E03R04_clip0358 -> positions 901 to 1259\n",
      "    S01E03R05: Processing 268 clips using FC file, starting at position 1260\n",
      "    S01E03R05: Clips S01E03R05_clip0000 to S01E03R05_clip0267 -> positions 1260 to 1527\n",
      "    S01E03R06: Processing 416 clips using FC file, starting at position 1528\n",
      "    S01E03R06: Clips S01E03R06_clip0000 to S01E03R06_clip0415 -> positions 1528 to 1943\n",
      "  Total clips for S01E03: 1943\n",
      "  Episode positions range: 1 to 1943\n",
      "  Sample position mapping: [('S01E03R01_clip0000', 1), ('S01E03R01_clip0001', 2), ('S01E03R01_clip0002', 3)]\n",
      "  Mapped 22/22 positions for S01E03\n",
      "\n",
      "Processing episode positions for S01E04...\n",
      "  Found prefixes for S01E04: ['S01E04R01', 'S01E04R02', 'S01E04R03', 'S01E04R04', 'S01E04R05']\n",
      "    S01E04R01: Processing 307 clips using AT file, starting at position 1\n",
      "    S01E04R01: Clips S01E04R01_clip0000 to S01E04R01_clip0306 -> positions 1 to 307\n",
      "    S01E04R02: Processing 296 clips using AT file, starting at position 308\n",
      "    S01E04R02: Clips S01E04R02_clip0000 to S01E04R02_clip0295 -> positions 308 to 603\n",
      "    S01E04R03: Processing 293 clips using AT file, starting at position 604\n",
      "    S01E04R03: Clips S01E04R03_clip0000 to S01E04R03_clip0292 -> positions 604 to 896\n",
      "    S01E04R04: Processing 334 clips using AT file, starting at position 897\n",
      "    S01E04R04: Clips S01E04R04_clip0000 to S01E04R04_clip0333 -> positions 897 to 1230\n",
      "    S01E04R05: Processing 230 clips using AT file, starting at position 1231\n",
      "    S01E04R05: Clips S01E04R05_clip0000 to S01E04R05_clip0229 -> positions 1231 to 1460\n",
      "  Total clips for S01E04: 1460\n",
      "  Episode positions range: 1 to 1460\n",
      "  Sample position mapping: [('S01E04R01_clip0000', 1), ('S01E04R01_clip0001', 2), ('S01E04R01_clip0002', 3)]\n",
      "  Mapped 16/20 positions for S01E04\n",
      "  WARNING: 4 clips in S01E04 could not be mapped\n",
      "    Used clip_number 308 as fallback position for nan\n",
      "    Used clip_number 294 as fallback position for nan\n",
      "    Used clip_number 295 as fallback position for nan\n",
      "    Used clip_number 296 as fallback position for nan\n",
      "\n",
      "Saved removal log: dset/derivatives/annotations/removed_clips_log.csv\n",
      "Removal log contains episode_position column: True\n",
      "Episode positions mapped: 208/208 (100.0%)\n",
      "\n",
      "Sample episode positions:\n",
      "  S01E01:\n",
      "    S01E01R01_clip0029 (clip 29) -> episode position 30.0\n",
      "    S01E01R01_clip0030 (clip 30) -> episode position 31.0\n",
      "    S01E01R01_clip0031 (clip 31) -> episode position 32.0\n",
      "  S01E02:\n",
      "    S01E02R01_clip0000 (clip 0) -> episode position 1.0\n",
      "    S01E02R01_clip0014 (clip 14) -> episode position 15.0\n",
      "    S01E02R01_clip0031 (clip 31) -> episode position 32.0\n",
      "  S01E03:\n",
      "    S01E03R01_clip0021 (clip 21) -> episode position 22.0\n",
      "    S01E03R01_clip0022 (clip 22) -> episode position 23.0\n",
      "    S01E03R01_clip0034 (clip 34) -> episode position 35.0\n",
      "  S01E04:\n",
      "    nan (clip 308) -> episode position 308.0\n",
      "    nan (clip 294) -> episode position 294.0\n",
      "    nan (clip 295) -> episode position 295.0\n",
      "\n",
      "Removals by episode:\n",
      "  S01E01: 84 clips removed, 84 with positions\n",
      "  S01E02: 82 clips removed, 82 with positions\n",
      "  S01E03: 22 clips removed, 22 with positions\n",
      "  S01E04: 20 clips removed, 20 with positions\n"
     ]
    }
   ],
   "source": [
    "groups = find_annotation_files(ANNOT_DIR)\n",
    "\n",
    "# Process all files but organize by episode instead of saving individual run files\n",
    "episode_data = {}\n",
    "all_removals = []\n",
    "\n",
    "for prefix, files in groups.items():\n",
    "    out = combine_group(prefix, files)\n",
    "    if out is None:\n",
    "        print(f\"No valid files for {prefix}\")\n",
    "        continue\n",
    "    \n",
    "    merged, renamed, removed_clips = out\n",
    "    \n",
    "    # Extract episode from prefix (e.g., S01E01R01 -> S01E01)\n",
    "    episode_match = re.match(r\"(S\\d+E\\d+)R\\d+\", prefix)\n",
    "    if episode_match:\n",
    "        episode = episode_match.group(1)\n",
    "        \n",
    "        # Add run identifier column\n",
    "        merged['run'] = prefix\n",
    "        \n",
    "        # Store in episode data structure\n",
    "        if episode not in episode_data:\n",
    "            episode_data[episode] = []\n",
    "        episode_data[episode].append((prefix, merged, len(files), len(removed_clips)))\n",
    "    \n",
    "    # Track removals for documentation\n",
    "    if removed_clips:\n",
    "        all_removals.extend(removed_clips)\n",
    "    \n",
    "    print(f\"Processed {prefix} ({len(files)} files -> {len(merged)} rows, {len(removed_clips) if removed_clips else 0} clips removed)\")\n",
    "\n",
    "# Create episode-level CSV files directly\n",
    "print(f\"\\nCreating episode-level files for {len(episode_data)} episodes...\")\n",
    "episode_summary = []\n",
    "\n",
    "for episode, run_data_list in episode_data.items():\n",
    "    print(f\"\\nCombining runs for {episode}:\")\n",
    "    \n",
    "    episode_dfs = []\n",
    "    for run_prefix, run_df, file_count, removed_count in sorted(run_data_list):\n",
    "        episode_dfs.append(run_df)\n",
    "        print(f\"  Added {run_prefix}: {len(run_df)} clips ({file_count} files, {removed_count} clips removed)\")\n",
    "    \n",
    "    if episode_dfs:\n",
    "        # Concatenate all runs for this episode\n",
    "        episode_combined = pd.concat(episode_dfs, ignore_index=True)\n",
    "        \n",
    "        # Reorder columns to put run first, then index, then emotions\n",
    "        val_cols = [c for c in episode_combined.columns if c.startswith(\"valence_\")]\n",
    "        aro_cols = [c for c in episode_combined.columns if c.startswith(\"arousal_\")]\n",
    "        other_cols = [c for c in episode_combined.columns if c not in ['run', 'index'] + val_cols + aro_cols]\n",
    "        \n",
    "        new_order = ['run', 'index'] + val_cols + aro_cols + other_cols\n",
    "        episode_combined = episode_combined[new_order]\n",
    "        \n",
    "        # Save episode-level file\n",
    "        episode_path = OUT_DIR / f\"{episode}.csv\"\n",
    "        episode_combined.to_csv(episode_path, index=False)\n",
    "        \n",
    "        total_clips = len(episode_combined)\n",
    "        episode_summary.append((episode, len(run_data_list), episode_path, total_clips))\n",
    "        print(f\"  Saved {episode_path}: {total_clips} total clips across {len(run_data_list)} runs\")\n",
    "\n",
    "print(f\"\\nEpisode-level files created:\")\n",
    "for episode, run_count, file_path, clip_count in episode_summary:\n",
    "    print(f\"  {episode}: {run_count} runs, {clip_count} clips -> {file_path}\")\n",
    "\n",
    "print(f\"\\nTotal clips removed across all episodes: {len(all_removals)}\")\n",
    "\n",
    "# Save removal documentation\n",
    "if all_removals:\n",
    "    removal_df = pd.DataFrame(all_removals)\n",
    "    \n",
    "    # Debug: Check what columns we actually have\n",
    "    print(f\"\\nDEBUG: Removal dataframe columns: {list(removal_df.columns)}\")\n",
    "    print(f\"DEBUG: First few rows of removal data:\")\n",
    "    print(removal_df.head())\n",
    "    \n",
    "    # Calculate sequential position within each episode\n",
    "    removal_df_with_positions = []\n",
    "    \n",
    "    for episode in removal_df['episode'].unique():\n",
    "        print(f\"\\nProcessing episode positions for {episode}...\")\n",
    "        episode_removals = removal_df[removal_df['episode'] == episode].copy()\n",
    "        \n",
    "        # Build complete episode clip sequence from annotation files\n",
    "        # We need to get the CORRECT sequential order across runs\n",
    "        episode_prefixes = [prefix for prefix in groups.keys() if prefix.startswith(episode)]\n",
    "        print(f\"  Found prefixes for {episode}: {sorted(episode_prefixes)}\")\n",
    "        \n",
    "        # Build cumulative episode position mapping based on actual run sequence\n",
    "        episode_clip_positions = {}\n",
    "        cumulative_position = 1  # Start from 1 for episode positions\n",
    "        \n",
    "        for run_prefix in sorted(episode_prefixes):\n",
    "            files = groups[run_prefix]\n",
    "            # Use any available file for this run (prefer FC, then fallback to others)\n",
    "            fc_files = [f for f in files if f[1].name.endswith('_FC.csv')]\n",
    "            nc_files = [f for f in files if f[1].name.endswith('_NC.csv')]\n",
    "            other_files = [f for f in files if not f[1].name.endswith(('_FC.csv', '_NC.csv'))]\n",
    "            \n",
    "            # Choose the best available file\n",
    "            if fc_files:\n",
    "                ref_file = fc_files[0][1]\n",
    "                file_type = \"FC\"\n",
    "            elif nc_files:\n",
    "                ref_file = nc_files[0][1]\n",
    "                file_type = \"NC\"\n",
    "            elif other_files:\n",
    "                ref_file = other_files[0][1]\n",
    "                file_type = other_files[0][0]\n",
    "            else:\n",
    "                print(f\"    {run_prefix}: No annotation files found!\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(ref_file)\n",
    "            \n",
    "            # For this run, assign sequential positions\n",
    "            run_clips = df['index'].tolist()\n",
    "            print(f\"    {run_prefix}: Processing {len(run_clips)} clips using {file_type} file, starting at position {cumulative_position}\")\n",
    "            \n",
    "            for clip_index in run_clips:\n",
    "                episode_clip_positions[clip_index] = cumulative_position\n",
    "                cumulative_position += 1\n",
    "                \n",
    "            print(f\"    {run_prefix}: Clips {run_clips[0]} to {run_clips[-1]} -> positions {episode_clip_positions[run_clips[0]]} to {episode_clip_positions[run_clips[-1]]}\")\n",
    "        \n",
    "        print(f\"  Total clips for {episode}: {len(episode_clip_positions)}\")\n",
    "        print(f\"  Episode positions range: 1 to {cumulative_position - 1}\")\n",
    "        \n",
    "        # Sample of the mapping for verification\n",
    "        sample_clips = list(episode_clip_positions.items())[:3]\n",
    "        print(f\"  Sample position mapping: {sample_clips}\")\n",
    "        \n",
    "        # Add positions to removals\n",
    "        if 'index' in episode_removals.columns:\n",
    "            episode_removals['episode_position'] = episode_removals['index'].map(episode_clip_positions)\n",
    "            \n",
    "            # Check mapping success\n",
    "            mapped_count = episode_removals['episode_position'].notna().sum()\n",
    "            total_count = len(episode_removals)\n",
    "            print(f\"  Mapped {mapped_count}/{total_count} positions for {episode}\")\n",
    "            \n",
    "            # Fill any NaN positions with fallback logic\n",
    "            nan_mask = episode_removals['episode_position'].isna()\n",
    "            if nan_mask.any():\n",
    "                print(f\"  WARNING: {nan_mask.sum()} clips in {episode} could not be mapped\")\n",
    "                \n",
    "                # For clips that couldn't be mapped, use clip number as fallback\n",
    "                for idx, row in episode_removals[nan_mask].iterrows():\n",
    "                    if pd.isna(row['episode_position']):\n",
    "                        # Try to use clip_number directly as fallback\n",
    "                        if pd.notna(row['clip_number']):\n",
    "                            episode_removals.loc[idx, 'episode_position'] = float(row['clip_number'])\n",
    "                            print(f\"    Used clip_number {row['clip_number']} as fallback position for {row['index']}\")\n",
    "                        else:\n",
    "                            print(f\"    Could not determine position for {row['index']}\")\n",
    "        else:\n",
    "            print(f\"  WARNING: 'index' column not found in episode_removals for {episode}\")\n",
    "            print(f\"  Available columns: {list(episode_removals.columns)}\")\n",
    "            # Use clip_number as fallback\n",
    "            episode_removals['episode_position'] = episode_removals['clip_number']\n",
    "        \n",
    "        removal_df_with_positions.append(episode_removals)\n",
    "    \n",
    "    # Combine all episodes\n",
    "    final_removal_df = pd.concat(removal_df_with_positions, ignore_index=True)\n",
    "    \n",
    "    # Ensure episode_position is preserved and properly formatted\n",
    "    if 'episode_position' in final_removal_df.columns:\n",
    "        # Convert to numeric where possible, keeping NaN as NaN\n",
    "        final_removal_df['episode_position'] = pd.to_numeric(final_removal_df['episode_position'], errors='coerce')\n",
    "        \n",
    "        # Final check for any remaining NaN positions and use clip_number as ultimate fallback\n",
    "        remaining_nan = final_removal_df['episode_position'].isna()\n",
    "        if remaining_nan.any():\n",
    "            print(f\"\\nFinal fallback: Using clip_number for {remaining_nan.sum()} remaining NaN positions\")\n",
    "            final_removal_df.loc[remaining_nan, 'episode_position'] = final_removal_df.loc[remaining_nan, 'clip_number']\n",
    "    else:\n",
    "        print(\"ERROR: episode_position column missing from final dataframe!\")\n",
    "        print(f\"Available columns: {list(final_removal_df.columns)}\")\n",
    "    \n",
    "    # Save removal log\n",
    "    removal_path = OUT_DIR / \"removed_clips_log.csv\"\n",
    "    final_removal_df.to_csv(removal_path, index=False)\n",
    "    print(f\"\\nSaved removal log: {removal_path}\")\n",
    "    print(f\"Removal log contains episode_position column: {'episode_position' in final_removal_df.columns}\")\n",
    "    \n",
    "    # Verify episode_position values\n",
    "    if 'episode_position' in final_removal_df.columns:\n",
    "        non_null_positions = final_removal_df['episode_position'].notna().sum()\n",
    "        total_removals = len(final_removal_df)\n",
    "        print(f\"Episode positions mapped: {non_null_positions}/{total_removals} ({non_null_positions/total_removals*100:.1f}%)\")\n",
    "        \n",
    "        # Show sample of episode positions by episode\n",
    "        print(f\"\\nSample episode positions:\")\n",
    "        for episode in sorted(final_removal_df['episode'].unique()):\n",
    "            episode_data = final_removal_df[final_removal_df['episode'] == episode]\n",
    "            sample_positions = episode_data[['index', 'clip_number', 'episode_position']].head(3)\n",
    "            print(f\"  {episode}:\")\n",
    "            for _, row in sample_positions.iterrows():\n",
    "                print(f\"    {row['index']} (clip {row['clip_number']}) -> episode position {row['episode_position']}\")\n",
    "    \n",
    "    # Print summary by episode\n",
    "    print(f\"\\nRemovals by episode:\")\n",
    "    for episode in sorted(removal_df['episode'].unique()):\n",
    "        episode_removals = final_removal_df[final_removal_df['episode'] == episode]\n",
    "        mapped_positions = episode_removals['episode_position'].notna().sum() if 'episode_position' in final_removal_df.columns else 0\n",
    "        print(f\"  {episode}: {len(episode_removals)} clips removed, {mapped_positions} with positions\")\n",
    "        \n",
    "else:\n",
    "    print(\"No clips were removed during processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9cfa9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode3_df = pd.read_csv(op.join(OUT_DIR, \"S01E03.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37afae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENHANCED REMOVAL LOG WITH EPISODE POSITIONS\n",
      "================================================================================\n",
      "Total removed clips: 208\n",
      "Columns: ['episode', 'run', 'run_prefix', 'index', 'clip_number', 'valence_ratings', 'arousal_ratings', 'valence_invalid_scores', 'arousal_invalid_scores', 'reason', 'episode_position']\n",
      "\n",
      "============================================================\n",
      "SAMPLE OF REMOVAL LOG (first 10 entries):\n",
      "============================================================\n",
      "episode  run run_prefix              index  clip_number  valence_ratings  arousal_ratings  valence_invalid_scores  arousal_invalid_scores                                                       reason  episode_position\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0029           29                2                1                       0                       1 insufficient arousal ratings (1); arousal scores > 7: [10.0]              30.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0030           30                2                1                       0                       1 insufficient arousal ratings (1); arousal scores > 7: [10.0]              31.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0031           31                2                1                       0                       1  insufficient arousal ratings (1); arousal scores > 7: [9.0]              32.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0032           32                2                1                       0                       1  insufficient arousal ratings (1); arousal scores > 7: [9.0]              33.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0033           33                2                1                       0                       1  insufficient arousal ratings (1); arousal scores > 7: [9.0]              34.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0034           34                2                1                       0                       1  insufficient arousal ratings (1); arousal scores > 7: [8.0]              35.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0035           35                2                1                       0                       1  insufficient arousal ratings (1); arousal scores > 7: [8.0]              36.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0036           36                2                1                       0                       1  insufficient arousal ratings (1); arousal scores > 7: [8.0]              37.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0037           37                2                1                       0                       1  insufficient arousal ratings (1); arousal scores > 7: [8.0]              38.0\n",
      " S01E01    1  S01E01R01 S01E01R01_clip0040           40                2                1                       0                       1  insufficient arousal ratings (1); arousal scores > 7: [8.0]              41.0\n",
      "\n",
      "============================================================\n",
      "EPISODE POSITION EXAMPLES:\n",
      "============================================================\n",
      "\n",
      "S01E01 (showing first 5 removals):\n",
      "  Position  30: S01E01R01_clip0029 - insufficient arousal ratings (1); arousal scores > 7: [10.0]\n",
      "  Position  31: S01E01R01_clip0030 - insufficient arousal ratings (1); arousal scores > 7: [10.0]\n",
      "  Position  32: S01E01R01_clip0031 - insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "  Position  33: S01E01R01_clip0032 - insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "  Position  34: S01E01R01_clip0033 - insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "\n",
      "S01E02 (showing first 5 removals):\n",
      "  Position   1: S01E02R01_clip0000 - insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "  Position  15: S01E02R01_clip0014 - insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "  Position  32: S01E02R01_clip0031 - insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "  Position 179: S01E02R01_clip0178 - insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "  Position 180: S01E02R01_clip0179 - insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "\n",
      "============================================================\n",
      "EPISODE SUMMARY:\n",
      "============================================================\n",
      "S01E01:  84/885 removed (  9.5%)\n",
      "S01E02:  82/2086 removed (  3.9%)\n",
      "S01E03:  22/1943 removed (  1.1%)\n",
      "S01E04:  20/1464 removed (  1.4%)\n"
     ]
    }
   ],
   "source": [
    "# Display the enhanced removal log with episode positions\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists(OUT_DIR / \"removed_clips_log.csv\"):\n",
    "    removal_log = pd.read_csv(OUT_DIR / \"removed_clips_log.csv\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ENHANCED REMOVAL LOG WITH EPISODE POSITIONS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total removed clips: {len(removal_log)}\")\n",
    "    print(f\"Columns: {list(removal_log.columns)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE OF REMOVAL LOG (first 10 entries):\")\n",
    "    print(\"=\"*60)\n",
    "    print(removal_log.head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EPISODE POSITION EXAMPLES:\")\n",
    "    print(\"=\"*60)\n",
    "    for episode in sorted(removal_log['episode'].unique())[:2]:  # Show first 2 episodes\n",
    "        episode_data = removal_log[removal_log['episode'] == episode].head(5)\n",
    "        print(f\"\\n{episode} (showing first 5 removals):\")\n",
    "        for _, row in episode_data.iterrows():\n",
    "            pos = row['episode_position']\n",
    "            pos_str = f\"{pos:.0f}\" if pd.notna(pos) else \"?\"\n",
    "            print(f\"  Position {pos_str:>3}: {row['index']} - {row['reason']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EPISODE SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    episode_stats = removal_log.groupby('episode').size().reset_index(name='removed_count')\n",
    "    \n",
    "    # Calculate total clips per episode from the episode CSV files  \n",
    "    for _, row in episode_stats.iterrows():\n",
    "        episode_name = row['episode']\n",
    "        removed_count = row['removed_count']\n",
    "        \n",
    "        # Get total clips from the episode file\n",
    "        episode_file = OUT_DIR / f\"{episode_name}.csv\"\n",
    "        if episode_file.exists():\n",
    "            episode_df = pd.read_csv(episode_file)\n",
    "            # Total clips = kept clips + removed clips\n",
    "            kept_clips = len(episode_df)\n",
    "            total_clips = kept_clips + removed_count\n",
    "            percentage = (removed_count / total_clips * 100) if total_clips > 0 else 0\n",
    "            print(f\"{episode_name}: {removed_count:3d}/{total_clips:3d} removed ({percentage:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"{episode_name}: {removed_count:3d}/??? removed (episode file not found)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No removal log found. Run the annotation processing first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
