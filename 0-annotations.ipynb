{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3effd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca807dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ANNOT_DIR = Path(\"dset/annotations\")\n",
    "OUT_DIR = Path(\"dset/derivatives/annotations\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bd745e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_annotation_files(annotation_dir: Path):\n",
    "    pattern = re.compile(r\"^(S\\d+E\\d+R\\d+)_([A-Za-z]{2})\\.csv$\")\n",
    "    groups = {}\n",
    "    for p in sorted(annotation_dir.iterdir()):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        m = pattern.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        prefix = m.group(1)\n",
    "        annot = m.group(2)\n",
    "        groups.setdefault(prefix, []).append((annot, p))\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02935427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_columns(df: pd.DataFrame):\n",
    "    # detect index column\n",
    "    if \"index\" in df.columns:\n",
    "        idx_col = \"index\"\n",
    "    else:\n",
    "        # fallback: first column\n",
    "        idx_col = df.columns[0]\n",
    "\n",
    "    # detect valence and arousal columns (case-insensitive)\n",
    "    val_cols = [c for c in df.columns if \"valence\" in c.lower()]\n",
    "    aro_cols = [c for c in df.columns if \"arousal\" in c.lower()]\n",
    "\n",
    "    val_col = val_cols[0] if val_cols else None\n",
    "    aro_col = aro_cols[0] if aro_cols else None\n",
    "    return idx_col, val_col, aro_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eae54c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_group(prefix, files):\n",
    "    # files: list of (annotator_code, Path)\n",
    "    \n",
    "    # Skip if less than 2 files\n",
    "    if len(files) < 2:\n",
    "        print(f\"Skipping {prefix}: need at least 2 files, found {len(files)}\")\n",
    "        return None\n",
    "    \n",
    "    dfs = []\n",
    "    renamed = []\n",
    "    valid_count = 0  # Track valid files separately from loop index\n",
    "    \n",
    "    for annot, path in sorted(files):\n",
    "        df = pd.read_csv(path)\n",
    "        idx_col, val_col, aro_col = detect_columns(df)\n",
    "        if val_col is None or aro_col is None:\n",
    "            print(f\"Skipping {path.name}: could not find valence/arousal columns\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        # create minimal df using valid_count for consistent numbering\n",
    "        valid_count += 1\n",
    "        out = pd.DataFrame()\n",
    "        out[\"index\"] = df[idx_col]\n",
    "        out[f\"valence_{valid_count}\"] = df[val_col].values\n",
    "        out[f\"arousal_{valid_count}\"] = df[aro_col].values\n",
    "        dfs.append(out)\n",
    "        renamed.append((annot, f\"valence_{valid_count}\", f\"arousal_{valid_count}\"))\n",
    "\n",
    "    if len(dfs) < 2:\n",
    "        print(f\"Skipping {prefix}: need at least 2 valid files after filtering, found {len(dfs)}\")\n",
    "        return None\n",
    "\n",
    "    # merge on index\n",
    "    merged = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        merged = pd.merge(merged, d, on=\"index\", how=\"outer\")\n",
    "\n",
    "    # sort by index if numeric\n",
    "    try:\n",
    "        merged[\"index\"] = pd.to_numeric(merged[\"index\"])\n",
    "        merged = merged.sort_values(\"index\").reset_index(drop=True)\n",
    "    except Exception:\n",
    "        merged = merged.reset_index(drop=True)\n",
    "\n",
    "    # Reorder columns: index, all valence_*, then all arousal_*\n",
    "    val_cols = [c for c in merged.columns if c.startswith(\"valence_\")]\n",
    "    aro_cols = [c for c in merged.columns if c.startswith(\"arousal_\")]\n",
    "\n",
    "    def _num_suffix(colname):\n",
    "        parts = colname.split(\"_\")\n",
    "        try:\n",
    "            return int(parts[-1])\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    val_cols = sorted(val_cols, key=_num_suffix)\n",
    "    aro_cols = sorted(aro_cols, key=_num_suffix)\n",
    "\n",
    "    new_cols = [\"index\"] + val_cols + aro_cols\n",
    "    # keep any unexpected columns at the end (shouldn't typically occur)\n",
    "    tail = [c for c in merged.columns if c not in new_cols]\n",
    "    merged = merged[new_cols + tail]\n",
    "\n",
    "    # Remove rows with insufficient ratings and document removals\n",
    "    initial_rows = len(merged)\n",
    "    removed_clips = []\n",
    "    \n",
    "    # Check each row for sufficient ratings and valid scores\n",
    "    rows_to_keep = []\n",
    "    for i, row in merged.iterrows():\n",
    "        # Count non-null valence ratings and check for scores > 7\n",
    "        val_ratings = []\n",
    "        val_invalid_scores = []\n",
    "        for col in val_cols:\n",
    "            if pd.notna(row[col]):\n",
    "                try:\n",
    "                    score = float(row[col])  # Convert to numeric\n",
    "                    if score > 7:\n",
    "                        val_invalid_scores.append(score)\n",
    "                    else:\n",
    "                        val_ratings.append(score)\n",
    "                except (ValueError, TypeError):\n",
    "                    # Skip non-numeric values\n",
    "                    print(f\"Warning: Non-numeric valence value {row[col]} in {col} for index {row['index']}\")\n",
    "                    continue\n",
    "        \n",
    "        # Count non-null arousal ratings and check for scores > 7\n",
    "        aro_ratings = []\n",
    "        aro_invalid_scores = []\n",
    "        for col in aro_cols:\n",
    "            if pd.notna(row[col]):\n",
    "                try:\n",
    "                    score = float(row[col])  # Convert to numeric\n",
    "                    if score > 7:\n",
    "                        aro_invalid_scores.append(score)\n",
    "                    else:\n",
    "                        aro_ratings.append(score)\n",
    "                except (ValueError, TypeError):\n",
    "                    # Skip non-numeric values\n",
    "                    print(f\"Warning: Non-numeric arousal value {row[col]} in {col} for index {row['index']}\")\n",
    "                    continue\n",
    "        \n",
    "        # Determine removal reason\n",
    "        removal_reasons = []\n",
    "        if len(val_ratings) < 2:\n",
    "            removal_reasons.append(f\"insufficient valence ratings ({len(val_ratings)})\")\n",
    "        if len(aro_ratings) < 2:\n",
    "            removal_reasons.append(f\"insufficient arousal ratings ({len(aro_ratings)})\")\n",
    "        if val_invalid_scores:\n",
    "            removal_reasons.append(f\"valence scores > 7: {val_invalid_scores}\")\n",
    "        if aro_invalid_scores:\n",
    "            removal_reasons.append(f\"arousal scores > 7: {aro_invalid_scores}\")\n",
    "        \n",
    "        # Keep row only if it has at least 2 valid ratings for both valence and arousal\n",
    "        if len(val_ratings) >= 2 and len(aro_ratings) >= 2 and not val_invalid_scores and not aro_invalid_scores:\n",
    "            rows_to_keep.append(i)\n",
    "        else:\n",
    "            removed_clips.append({\n",
    "                'index': row['index'],\n",
    "                'valence_ratings': len(val_ratings),\n",
    "                'arousal_ratings': len(aro_ratings),\n",
    "                'valence_invalid_scores': len(val_invalid_scores),\n",
    "                'arousal_invalid_scores': len(aro_invalid_scores),\n",
    "                'reason': \"; \".join(removal_reasons)\n",
    "            })\n",
    "    \n",
    "    # Filter the dataframe\n",
    "    if rows_to_keep:\n",
    "        merged_filtered = merged.iloc[rows_to_keep].reset_index(drop=True)\n",
    "    else:\n",
    "        merged_filtered = pd.DataFrame(columns=merged.columns)\n",
    "    \n",
    "    final_rows = len(merged_filtered)\n",
    "    \n",
    "    # Report removals\n",
    "    if removed_clips:\n",
    "        print(f\"  Removed {len(removed_clips)}/{initial_rows} clips with insufficient ratings:\")\n",
    "        for clip in removed_clips[:5]:  # Show first 5 removals\n",
    "            print(f\"    - {clip['index']}: {clip['reason']}\")\n",
    "        if len(removed_clips) > 5:\n",
    "            print(f\"    ... and {len(removed_clips) - 5} more\")\n",
    "    \n",
    "    return merged_filtered, renamed, removed_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f8ec108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 36/334 clips with insufficient ratings:\n",
      "    - S01E01R01_clip0029: insufficient arousal ratings (1); arousal scores > 7: [10.0]\n",
      "    - S01E01R01_clip0030: insufficient arousal ratings (1); arousal scores > 7: [10.0]\n",
      "    - S01E01R01_clip0031: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E01R01_clip0032: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E01R01_clip0033: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    ... and 31 more\n",
      "Wrote dset/derivatives/annotations/S01E01R01.csv (2 files -> 298 rows)\n",
      "Wrote dset/derivatives/annotations/S01E01R02.csv (2 files -> 276 rows)\n",
      "  Removed 48/275 clips with insufficient ratings:\n",
      "    - S01E01R03_clip0147: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E01R03_clip0148: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E01R03_clip0149: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E01R03_clip0150: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E01R03_clip0151: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 43 more\n",
      "Wrote dset/derivatives/annotations/S01E01R03.csv (2 files -> 227 rows)\n",
      "Skipping S01E01R04: need at least 2 files, found 1\n",
      "No valid files for S01E01R04\n",
      "Skipping S01E01R05: need at least 2 files, found 1\n",
      "No valid files for S01E01R05\n",
      "Skipping S01E01R06: need at least 2 files, found 1\n",
      "No valid files for S01E01R06\n",
      "  Removed 10/313 clips with insufficient ratings:\n",
      "    - S01E02R01_clip0000: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "    - S01E02R01_clip0014: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R01_clip0031: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E02R01_clip0178: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "    - S01E02R01_clip0179: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "    ... and 5 more\n",
      "Wrote dset/derivatives/annotations/S01E02R01.csv (2 files -> 303 rows)\n",
      "  Removed 20/250 clips with insufficient ratings:\n",
      "    - S01E02R02_clip0015: arousal scores > 7: [9.0]\n",
      "    - S01E02R02_clip0016: arousal scores > 7: [9.0]\n",
      "    - S01E02R02_clip0026: valence scores > 7: [9.0]\n",
      "    - S01E02R02_clip0034: valence scores > 7: [8.0]\n",
      "    - S01E02R02_clip0035: valence scores > 7: [8.0]\n",
      "    ... and 15 more\n",
      "Wrote dset/derivatives/annotations/S01E02R02.csv (8 files -> 230 rows)\n",
      "  Removed 6/362 clips with insufficient ratings:\n",
      "    - S01E02R03_clip0058: insufficient valence ratings (1); valence scores > 7: [9.0]\n",
      "    - S01E02R03_clip0059: insufficient valence ratings (1); valence scores > 7: [9.0]\n",
      "    - S01E02R03_clip0175: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R03_clip0176: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R03_clip0177: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 1 more\n",
      "Wrote dset/derivatives/annotations/S01E02R03.csv (2 files -> 356 rows)\n",
      "  Removed 4/354 clips with insufficient ratings:\n",
      "    - S01E02R04_clip0162: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R04_clip0219: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R04_clip0224: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R04_clip0226: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "Wrote dset/derivatives/annotations/S01E02R04.csv (2 files -> 350 rows)\n",
      "  Removed 3/295 clips with insufficient ratings:\n",
      "    - S01E02R05_clip0048: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E02R05_clip0051: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R05_clip0052: insufficient arousal ratings (1); arousal scores > 7: [10.0]\n",
      "Wrote dset/derivatives/annotations/S01E02R05.csv (2 files -> 292 rows)\n",
      "  Removed 26/218 clips with insufficient ratings:\n",
      "    - S01E02R06_clip0082: insufficient arousal ratings (1)\n",
      "    - S01E02R06_clip0083: insufficient arousal ratings (1)\n",
      "    - S01E02R06_clip0084: insufficient arousal ratings (1)\n",
      "    - S01E02R06_clip0085: insufficient arousal ratings (1)\n",
      "    - S01E02R06_clip0086: insufficient arousal ratings (1)\n",
      "    ... and 21 more\n",
      "Wrote dset/derivatives/annotations/S01E02R06.csv (2 files -> 192 rows)\n",
      "  Removed 13/294 clips with insufficient ratings:\n",
      "    - S01E02R07_clip0027: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E02R07_clip0033: insufficient arousal ratings (0); arousal scores > 7: [9.0, 9.0]\n",
      "    - S01E02R07_clip0068: insufficient arousal ratings (0); arousal scores > 7: [9.0, 8.0]\n",
      "    - S01E02R07_clip0102: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E02R07_clip0103: insufficient arousal ratings (0); arousal scores > 7: [9.0, 8.0]\n",
      "    ... and 8 more\n",
      "Wrote dset/derivatives/annotations/S01E02R07.csv (2 files -> 281 rows)\n",
      "  Removed 11/272 clips with insufficient ratings:\n",
      "    - S01E03R01_clip0021: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E03R01_clip0022: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E03R01_clip0034: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E03R01_clip0051: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    - S01E03R01_clip0055: insufficient arousal ratings (1); arousal scores > 7: [9.0]\n",
      "    ... and 6 more\n",
      "Wrote dset/derivatives/annotations/S01E03R01.csv (2 files -> 261 rows)\n",
      "  Removed 2/311 clips with insufficient ratings:\n",
      "    - S01E03R02_clip0307: insufficient arousal ratings (1)\n",
      "    - S01E03R02_clip0310: insufficient valence ratings (1); insufficient arousal ratings (1)\n",
      "Wrote dset/derivatives/annotations/S01E03R02.csv (2 files -> 309 rows)\n",
      "Warning: Non-numeric valence value \\ in valence_1 for index S01E03R03_clip0137\n",
      "  Removed 1/317 clips with insufficient ratings:\n",
      "    - S01E03R03_clip0137: insufficient valence ratings (1)\n",
      "Wrote dset/derivatives/annotations/S01E03R03.csv (2 files -> 316 rows)\n",
      "Wrote dset/derivatives/annotations/S01E03R04.csv (2 files -> 359 rows)\n",
      "Wrote dset/derivatives/annotations/S01E03R05.csv (2 files -> 268 rows)\n",
      "  Removed 8/416 clips with insufficient ratings:\n",
      "    - S01E03R06_clip0276: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0278: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0279: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0280: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0281: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 3 more\n",
      "Wrote dset/derivatives/annotations/S01E03R06.csv (2 files -> 408 rows)\n",
      "  Removed 1/308 clips with insufficient ratings:\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "Wrote dset/derivatives/annotations/S01E04R01.csv (2 files -> 307 rows)\n",
      "Wrote dset/derivatives/annotations/S01E04R02.csv (2 files -> 296 rows)\n",
      "  Removed 3/296 clips with insufficient ratings:\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "Wrote dset/derivatives/annotations/S01E04R03.csv (2 files -> 293 rows)\n",
      "  Removed 16/334 clips with insufficient ratings:\n",
      "    - S01E04R04_clip0219: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0220: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0221: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0222: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0223: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 11 more\n",
      "Wrote dset/derivatives/annotations/S01E04R04.csv (2 files -> 318 rows)\n",
      "Wrote dset/derivatives/annotations/S01E04R05.csv (2 files -> 230 rows)\n",
      "\n",
      "Finished. Combined groups:\n",
      "  S01E01R01: 2 files -> dset/derivatives/annotations/S01E01R01.csv (36 clips removed)\n",
      "  S01E01R02: 2 files -> dset/derivatives/annotations/S01E01R02.csv (0 clips removed)\n",
      "  S01E01R03: 2 files -> dset/derivatives/annotations/S01E01R03.csv (48 clips removed)\n",
      "  S01E02R01: 2 files -> dset/derivatives/annotations/S01E02R01.csv (10 clips removed)\n",
      "  S01E02R02: 8 files -> dset/derivatives/annotations/S01E02R02.csv (20 clips removed)\n",
      "  S01E02R03: 2 files -> dset/derivatives/annotations/S01E02R03.csv (6 clips removed)\n",
      "  S01E02R04: 2 files -> dset/derivatives/annotations/S01E02R04.csv (4 clips removed)\n",
      "  S01E02R05: 2 files -> dset/derivatives/annotations/S01E02R05.csv (3 clips removed)\n",
      "  S01E02R06: 2 files -> dset/derivatives/annotations/S01E02R06.csv (26 clips removed)\n",
      "  S01E02R07: 2 files -> dset/derivatives/annotations/S01E02R07.csv (13 clips removed)\n",
      "  S01E03R01: 2 files -> dset/derivatives/annotations/S01E03R01.csv (11 clips removed)\n",
      "  S01E03R02: 2 files -> dset/derivatives/annotations/S01E03R02.csv (2 clips removed)\n",
      "  S01E03R03: 2 files -> dset/derivatives/annotations/S01E03R03.csv (1 clips removed)\n",
      "  S01E03R04: 2 files -> dset/derivatives/annotations/S01E03R04.csv (0 clips removed)\n",
      "  S01E03R05: 2 files -> dset/derivatives/annotations/S01E03R05.csv (0 clips removed)\n",
      "  S01E03R06: 2 files -> dset/derivatives/annotations/S01E03R06.csv (8 clips removed)\n",
      "  S01E04R01: 2 files -> dset/derivatives/annotations/S01E04R01.csv (1 clips removed)\n",
      "  S01E04R02: 2 files -> dset/derivatives/annotations/S01E04R02.csv (0 clips removed)\n",
      "  S01E04R03: 2 files -> dset/derivatives/annotations/S01E04R03.csv (3 clips removed)\n",
      "  S01E04R04: 2 files -> dset/derivatives/annotations/S01E04R04.csv (16 clips removed)\n",
      "  S01E04R05: 2 files -> dset/derivatives/annotations/S01E04R05.csv (0 clips removed)\n",
      "\n",
      "Removal log saved to: dset/derivatives/annotations/removed_clips_log.csv\n",
      "Total clips removed across all runs: 208\n",
      "\n",
      "Removal reasons:\n",
      "  arousal scores > 7: [8.0]: 13 clips\n",
      "  arousal scores > 7: [9.0]: 4 clips\n",
      "  insufficient arousal ratings (0); arousal scores > 7: [9.0, 8.0]: 2 clips\n",
      "  insufficient arousal ratings (0); arousal scores > 7: [9.0, 9.0]: 2 clips\n",
      "  insufficient arousal ratings (1): 26 clips\n",
      "  insufficient arousal ratings (1); arousal scores > 7: [10.0]: 4 clips\n",
      "  insufficient arousal ratings (1); arousal scores > 7: [8.0]: 99 clips\n",
      "  insufficient arousal ratings (1); arousal scores > 7: [9.0]: 37 clips\n",
      "  insufficient valence ratings (0); insufficient arousal ratings (0): 4 clips\n",
      "  insufficient valence ratings (1): 1 clips\n",
      "  insufficient valence ratings (1); insufficient arousal ratings (1): 11 clips\n",
      "  insufficient valence ratings (1); valence scores > 7: [9.0]: 2 clips\n",
      "  valence scores > 7: [8.0]: 2 clips\n",
      "  valence scores > 7: [9.0]: 1 clips\n",
      "Wrote dset/derivatives/annotations/S01E03R05.csv (2 files -> 268 rows)\n",
      "  Removed 8/416 clips with insufficient ratings:\n",
      "    - S01E03R06_clip0276: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0278: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0279: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0280: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E03R06_clip0281: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 3 more\n",
      "Wrote dset/derivatives/annotations/S01E03R06.csv (2 files -> 408 rows)\n",
      "  Removed 1/308 clips with insufficient ratings:\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "Wrote dset/derivatives/annotations/S01E04R01.csv (2 files -> 307 rows)\n",
      "Wrote dset/derivatives/annotations/S01E04R02.csv (2 files -> 296 rows)\n",
      "  Removed 3/296 clips with insufficient ratings:\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "    - nan: insufficient valence ratings (0); insufficient arousal ratings (0)\n",
      "Wrote dset/derivatives/annotations/S01E04R03.csv (2 files -> 293 rows)\n",
      "  Removed 16/334 clips with insufficient ratings:\n",
      "    - S01E04R04_clip0219: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0220: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0221: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0222: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    - S01E04R04_clip0223: insufficient arousal ratings (1); arousal scores > 7: [8.0]\n",
      "    ... and 11 more\n",
      "Wrote dset/derivatives/annotations/S01E04R04.csv (2 files -> 318 rows)\n",
      "Wrote dset/derivatives/annotations/S01E04R05.csv (2 files -> 230 rows)\n",
      "\n",
      "Finished. Combined groups:\n",
      "  S01E01R01: 2 files -> dset/derivatives/annotations/S01E01R01.csv (36 clips removed)\n",
      "  S01E01R02: 2 files -> dset/derivatives/annotations/S01E01R02.csv (0 clips removed)\n",
      "  S01E01R03: 2 files -> dset/derivatives/annotations/S01E01R03.csv (48 clips removed)\n",
      "  S01E02R01: 2 files -> dset/derivatives/annotations/S01E02R01.csv (10 clips removed)\n",
      "  S01E02R02: 8 files -> dset/derivatives/annotations/S01E02R02.csv (20 clips removed)\n",
      "  S01E02R03: 2 files -> dset/derivatives/annotations/S01E02R03.csv (6 clips removed)\n",
      "  S01E02R04: 2 files -> dset/derivatives/annotations/S01E02R04.csv (4 clips removed)\n",
      "  S01E02R05: 2 files -> dset/derivatives/annotations/S01E02R05.csv (3 clips removed)\n",
      "  S01E02R06: 2 files -> dset/derivatives/annotations/S01E02R06.csv (26 clips removed)\n",
      "  S01E02R07: 2 files -> dset/derivatives/annotations/S01E02R07.csv (13 clips removed)\n",
      "  S01E03R01: 2 files -> dset/derivatives/annotations/S01E03R01.csv (11 clips removed)\n",
      "  S01E03R02: 2 files -> dset/derivatives/annotations/S01E03R02.csv (2 clips removed)\n",
      "  S01E03R03: 2 files -> dset/derivatives/annotations/S01E03R03.csv (1 clips removed)\n",
      "  S01E03R04: 2 files -> dset/derivatives/annotations/S01E03R04.csv (0 clips removed)\n",
      "  S01E03R05: 2 files -> dset/derivatives/annotations/S01E03R05.csv (0 clips removed)\n",
      "  S01E03R06: 2 files -> dset/derivatives/annotations/S01E03R06.csv (8 clips removed)\n",
      "  S01E04R01: 2 files -> dset/derivatives/annotations/S01E04R01.csv (1 clips removed)\n",
      "  S01E04R02: 2 files -> dset/derivatives/annotations/S01E04R02.csv (0 clips removed)\n",
      "  S01E04R03: 2 files -> dset/derivatives/annotations/S01E04R03.csv (3 clips removed)\n",
      "  S01E04R04: 2 files -> dset/derivatives/annotations/S01E04R04.csv (16 clips removed)\n",
      "  S01E04R05: 2 files -> dset/derivatives/annotations/S01E04R05.csv (0 clips removed)\n",
      "\n",
      "Removal log saved to: dset/derivatives/annotations/removed_clips_log.csv\n",
      "Total clips removed across all runs: 208\n",
      "\n",
      "Removal reasons:\n",
      "  arousal scores > 7: [8.0]: 13 clips\n",
      "  arousal scores > 7: [9.0]: 4 clips\n",
      "  insufficient arousal ratings (0); arousal scores > 7: [9.0, 8.0]: 2 clips\n",
      "  insufficient arousal ratings (0); arousal scores > 7: [9.0, 9.0]: 2 clips\n",
      "  insufficient arousal ratings (1): 26 clips\n",
      "  insufficient arousal ratings (1); arousal scores > 7: [10.0]: 4 clips\n",
      "  insufficient arousal ratings (1); arousal scores > 7: [8.0]: 99 clips\n",
      "  insufficient arousal ratings (1); arousal scores > 7: [9.0]: 37 clips\n",
      "  insufficient valence ratings (0); insufficient arousal ratings (0): 4 clips\n",
      "  insufficient valence ratings (1): 1 clips\n",
      "  insufficient valence ratings (1); insufficient arousal ratings (1): 11 clips\n",
      "  insufficient valence ratings (1); valence scores > 7: [9.0]: 2 clips\n",
      "  valence scores > 7: [8.0]: 2 clips\n",
      "  valence scores > 7: [9.0]: 1 clips\n"
     ]
    }
   ],
   "source": [
    "groups = find_annotation_files(ANNOT_DIR)\n",
    "\n",
    "summary = []\n",
    "all_removals = []\n",
    "\n",
    "for prefix, files in groups.items():\n",
    "    out = combine_group(prefix, files)\n",
    "    if out is None:\n",
    "        print(f\"No valid files for {prefix}\")\n",
    "        continue\n",
    "    \n",
    "    merged, renamed, removed_clips = out\n",
    "    \n",
    "    # Save the combined file\n",
    "    out_path = OUT_DIR / f\"{prefix}.csv\"\n",
    "    merged.to_csv(out_path, index=False)\n",
    "    print(f\"Wrote {out_path} ({len(files)} files -> {len(merged)} rows)\")\n",
    "    \n",
    "    # Track removals for documentation\n",
    "    if removed_clips:\n",
    "        for clip in removed_clips:\n",
    "            clip['run'] = prefix\n",
    "        all_removals.extend(removed_clips)\n",
    "    \n",
    "    summary.append((prefix, len(files), out_path, len(removed_clips)))\n",
    "\n",
    "print(\"\\nFinished. Combined groups:\")\n",
    "for s in summary:\n",
    "    print(f\"  {s[0]}: {s[1]} files -> {s[2]} ({s[3]} clips removed)\")\n",
    "\n",
    "# Save removal documentation\n",
    "if all_removals:\n",
    "    removal_df = pd.DataFrame(all_removals)\n",
    "    removal_path = OUT_DIR / \"removed_clips_log.csv\"\n",
    "    removal_df.to_csv(removal_path, index=False)\n",
    "    print(f\"\\nRemoval log saved to: {removal_path}\")\n",
    "    print(f\"Total clips removed across all runs: {len(all_removals)}\")\n",
    "    \n",
    "    # Summary by reason\n",
    "    removal_summary = removal_df.groupby('reason').size().reset_index(name='count')\n",
    "    print(f\"\\nRemoval reasons:\")\n",
    "    for _, row in removal_summary.iterrows():\n",
    "        print(f\"  {row['reason']}: {row['count']} clips\")\n",
    "else:\n",
    "    print(\"\\nNo clips were removed - all had sufficient ratings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3256e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 episodes:\n",
      "  S01E01: 3 runs\n",
      "  S01E02: 7 runs\n",
      "  S01E03: 6 runs\n",
      "  S01E04: 5 runs\n",
      "\n",
      "Combining runs for S01E01:\n",
      "  Added S01E01R01: 298 clips\n",
      "  Added S01E01R02: 276 clips\n",
      "  Added S01E01R03: 227 clips\n",
      "  Saved dset/derivatives/annotations/S01E01.csv: 801 total clips across 3 runs\n",
      "\n",
      "Combining runs for S01E02:\n",
      "  Added S01E02R01: 303 clips\n",
      "  Added S01E02R02: 230 clips\n",
      "  Added S01E02R03: 356 clips\n",
      "  Added S01E02R04: 350 clips\n",
      "  Added S01E02R05: 292 clips\n",
      "  Added S01E02R06: 192 clips\n",
      "  Added S01E02R07: 281 clips\n",
      "  Saved dset/derivatives/annotations/S01E02.csv: 2004 total clips across 7 runs\n",
      "\n",
      "Combining runs for S01E03:\n",
      "  Added S01E03R01: 261 clips\n",
      "  Added S01E03R02: 309 clips\n",
      "  Added S01E03R03: 316 clips\n",
      "  Added S01E03R04: 359 clips\n",
      "  Added S01E03R05: 268 clips\n",
      "  Added S01E03R06: 408 clips\n",
      "  Saved dset/derivatives/annotations/S01E03.csv: 1921 total clips across 6 runs\n",
      "\n",
      "Combining runs for S01E04:\n",
      "  Added S01E04R01: 307 clips\n",
      "  Added S01E04R02: 296 clips\n",
      "  Added S01E04R03: 293 clips\n",
      "  Added S01E04R04: 318 clips\n",
      "  Added S01E04R05: 230 clips\n",
      "  Saved dset/derivatives/annotations/S01E04.csv: 1444 total clips across 5 runs\n",
      "\n",
      "Episode-level files created:\n",
      "  S01E01: 3 runs, 801 clips -> dset/derivatives/annotations/S01E01.csv\n",
      "  S01E02: 7 runs, 2004 clips -> dset/derivatives/annotations/S01E02.csv\n",
      "  S01E03: 6 runs, 1921 clips -> dset/derivatives/annotations/S01E03.csv\n",
      "  S01E04: 5 runs, 1444 clips -> dset/derivatives/annotations/S01E04.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine runs by episode to create episode-level CSVs\n",
    "episode_groups = {}\n",
    "\n",
    "# Group the existing combined CSVs by episode\n",
    "for prefix, file_count, file_path, removed_count in summary:\n",
    "    # Extract episode from prefix (e.g., S01E01R01 -> S01E01)\n",
    "    episode_match = re.match(r\"(S\\d+E\\d+)R\\d+\", prefix)\n",
    "    if episode_match:\n",
    "        episode = episode_match.group(1)\n",
    "        episode_groups.setdefault(episode, []).append((prefix, file_path))\n",
    "\n",
    "print(f\"Found {len(episode_groups)} episodes:\")\n",
    "for episode, runs in episode_groups.items():\n",
    "    print(f\"  {episode}: {len(runs)} runs\")\n",
    "\n",
    "# Create combined episode files\n",
    "episode_summary = []\n",
    "for episode, runs in episode_groups.items():\n",
    "    print(f\"\\nCombining runs for {episode}:\")\n",
    "    \n",
    "    episode_dfs = []\n",
    "    for run_prefix, run_path in sorted(runs):\n",
    "        # Load the run data\n",
    "        run_df = pd.read_csv(run_path)\n",
    "        \n",
    "        # Add run identifier column\n",
    "        run_df['run'] = run_prefix\n",
    "        episode_dfs.append(run_df)\n",
    "        print(f\"  Added {run_prefix}: {len(run_df)} clips\")\n",
    "    \n",
    "    if episode_dfs:\n",
    "        # Concatenate all runs for this episode\n",
    "        episode_combined = pd.concat(episode_dfs, ignore_index=True)\n",
    "        \n",
    "        # Reorder columns to put run first, then index, then emotions\n",
    "        val_cols = [c for c in episode_combined.columns if c.startswith(\"valence_\")]\n",
    "        aro_cols = [c for c in episode_combined.columns if c.startswith(\"arousal_\")]\n",
    "        other_cols = [c for c in episode_combined.columns if c not in ['run', 'index'] + val_cols + aro_cols]\n",
    "        \n",
    "        new_order = ['run', 'index'] + val_cols + aro_cols + other_cols\n",
    "        episode_combined = episode_combined[new_order]\n",
    "        \n",
    "        # Save episode-level file\n",
    "        episode_path = OUT_DIR / f\"{episode}.csv\"\n",
    "        episode_combined.to_csv(episode_path, index=False)\n",
    "        \n",
    "        total_clips = len(episode_combined)\n",
    "        episode_summary.append((episode, len(runs), episode_path, total_clips))\n",
    "        print(f\"  Saved {episode_path}: {total_clips} total clips across {len(runs)} runs\")\n",
    "\n",
    "print(f\"\\nEpisode-level files created:\")\n",
    "for episode, run_count, file_path, clip_count in episode_summary:\n",
    "    print(f\"  {episode}: {run_count} runs, {clip_count} clips -> {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cfa9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode3_df = pd.read_csv(op.join(OUT_DIR, \"S01E03.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c03c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EPISODE 3 DATA ANALYSIS\n",
      "============================================================\n",
      "DataFrame shape: (1921, 6)\n",
      "Columns: ['run', 'index', 'valence_1', 'valence_2', 'arousal_1', 'arousal_2']\n",
      "Data types:\n",
      "run           object\n",
      "index         object\n",
      "valence_1    float64\n",
      "valence_2      int64\n",
      "arousal_1    float64\n",
      "arousal_2      int64\n",
      "dtype: object\n",
      "\n",
      "========================================\n",
      "SAMPLE DATA (first 10 rows):\n",
      "========================================\n",
      "         run               index  valence_1  valence_2  arousal_1  arousal_2\n",
      "0  S01E03R01  S01E03R01_clip0000        3.0          5        3.0          4\n",
      "1  S01E03R01  S01E03R01_clip0001        3.0          5        3.0          4\n",
      "2  S01E03R01  S01E03R01_clip0002        3.0          5        3.0          4\n",
      "3  S01E03R01  S01E03R01_clip0003        1.0          5        7.0          5\n",
      "4  S01E03R01  S01E03R01_clip0004        1.0          5        7.0          5\n",
      "5  S01E03R01  S01E03R01_clip0005        1.0          5        7.0          5\n",
      "6  S01E03R01  S01E03R01_clip0006        1.0          5        5.0          5\n",
      "7  S01E03R01  S01E03R01_clip0007        1.0          5        5.0          5\n",
      "8  S01E03R01  S01E03R01_clip0008        1.0          5        5.0          5\n",
      "9  S01E03R01  S01E03R01_clip0009        1.0          5        5.0          5\n",
      "\n",
      "========================================\n",
      "INDEX COLUMN ANALYSIS:\n",
      "========================================\n",
      "Index column type: <class 'str'>\n",
      "Index unique values (first 20): ['S01E03R01_clip0000' 'S01E03R01_clip0001' 'S01E03R01_clip0002'\n",
      " 'S01E03R01_clip0003' 'S01E03R01_clip0004' 'S01E03R01_clip0005'\n",
      " 'S01E03R01_clip0006' 'S01E03R01_clip0007' 'S01E03R01_clip0008'\n",
      " 'S01E03R01_clip0009' 'S01E03R01_clip0010' 'S01E03R01_clip0011'\n",
      " 'S01E03R01_clip0012' 'S01E03R01_clip0013' 'S01E03R01_clip0014'\n",
      " 'S01E03R01_clip0015' 'S01E03R01_clip0016' 'S01E03R01_clip0017'\n",
      " 'S01E03R01_clip0018' 'S01E03R01_clip0019']\n",
      "Any null values in index: 0\n",
      "\n",
      "========================================\n",
      "INDEX PROCESSING TEST:\n",
      "========================================\n",
      "Index as string (first 10): ['S01E03R01_clip0000', 'S01E03R01_clip0001', 'S01E03R01_clip0002', 'S01E03R01_clip0003', 'S01E03R01_clip0004', 'S01E03R01_clip0005', 'S01E03R01_clip0006', 'S01E03R01_clip0007', 'S01E03R01_clip0008', 'S01E03R01_clip0009']\n",
      "Last 4 characters (first 10): ['0000', '0001', '0002', '0003', '0004', '0005', '0006', '0007', '0008', '0009']\n",
      "Last 4 as int (first 10): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "✅ Index processing successful\n",
      "\n",
      "========================================\n",
      "EMOTION COLUMNS ANALYSIS:\n",
      "========================================\n",
      "Valence columns: ['valence_1', 'valence_2']\n",
      "Arousal columns: ['arousal_1', 'arousal_2']\n",
      "\n",
      "valence_1:\n",
      "  Type: float64\n",
      "  Unique values: [np.float64(1.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0)]\n",
      "  Null count: 0\n",
      "  Sample values: [3.0, 3.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "valence_2:\n",
      "  Type: int64\n",
      "  Unique values: [np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "  Null count: 0\n",
      "  Sample values: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "\n",
      "arousal_1:\n",
      "  Type: float64\n",
      "  Unique values: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(5.0), np.float64(7.0)]\n",
      "  Null count: 0\n",
      "  Sample values: [3.0, 3.0, 3.0, 7.0, 7.0, 7.0, 5.0, 5.0, 5.0, 5.0]\n",
      "\n",
      "arousal_2:\n",
      "  Type: int64\n",
      "  Unique values: [np.int64(4), np.int64(5), np.int64(6), np.int64(7)]\n",
      "  Null count: 0\n",
      "  Sample values: [4, 4, 4, 5, 5, 5, 5, 5, 5, 5]\n",
      "\n",
      "========================================\n",
      "RUN COLUMN ANALYSIS:\n",
      "========================================\n",
      "Run column unique values: ['S01E03R01' 'S01E03R02' 'S01E03R03' 'S01E03R04' 'S01E03R05' 'S01E03R06']\n",
      "Run value counts:\n",
      "run\n",
      "S01E03R06    408\n",
      "S01E03R04    359\n",
      "S01E03R03    316\n",
      "S01E03R02    309\n",
      "S01E03R05    268\n",
      "S01E03R01    261\n",
      "Name: count, dtype: int64\n",
      "\n",
      "========================================\n",
      "DATA QUALITY CHECKS:\n",
      "========================================\n",
      "Any completely empty rows: 0\n",
      "Any duplicate rows: 0\n",
      "\n",
      "Checking for mixed data types in emotion columns:\n",
      "  valence_1: ✅ All values convert to numeric properly\n",
      "  valence_2: ✅ All values convert to numeric properly\n",
      "  arousal_1: ✅ All values convert to numeric properly\n",
      "  arousal_2: ✅ All values convert to numeric properly\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive analysis of episode 3 data\n",
    "print(\"=\"*60)\n",
    "print(\"EPISODE 3 DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"DataFrame shape: {episode3_df.shape}\")\n",
    "print(f\"Columns: {list(episode3_df.columns)}\")\n",
    "print(f\"Data types:\\n{episode3_df.dtypes}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"SAMPLE DATA (first 10 rows):\")\n",
    "print(\"=\"*40)\n",
    "print(episode3_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"INDEX COLUMN ANALYSIS:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Index column type: {type(episode3_df['index'].iloc[0])}\")\n",
    "print(f\"Index unique values (first 20): {episode3_df['index'].unique()[:20]}\")\n",
    "print(f\"Any null values in index: {episode3_df['index'].isnull().sum()}\")\n",
    "\n",
    "# Try the index processing that caused issues\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"INDEX PROCESSING TEST:\")\n",
    "print(\"=\"*40)\n",
    "try:\n",
    "    index_as_str = episode3_df['index'].astype(str)\n",
    "    print(f\"Index as string (first 10): {index_as_str.head(10).tolist()}\")\n",
    "    \n",
    "    last_4_chars = index_as_str.str[-4:]\n",
    "    print(f\"Last 4 characters (first 10): {last_4_chars.head(10).tolist()}\")\n",
    "    \n",
    "    last_4_as_int = last_4_chars.astype(int)\n",
    "    print(f\"Last 4 as int (first 10): {last_4_as_int.head(10).tolist()}\")\n",
    "    print(\"✅ Index processing successful\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Index processing failed: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EMOTION COLUMNS ANALYSIS:\")\n",
    "print(\"=\"*40)\n",
    "val_cols = [col for col in episode3_df.columns if 'valence' in col.lower()]\n",
    "aro_cols = [col for col in episode3_df.columns if 'arousal' in col.lower()]\n",
    "\n",
    "print(f\"Valence columns: {val_cols}\")\n",
    "print(f\"Arousal columns: {aro_cols}\")\n",
    "\n",
    "for col in val_cols + aro_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Type: {episode3_df[col].dtype}\")\n",
    "    print(f\"  Unique values: {sorted(episode3_df[col].unique())}\")\n",
    "    print(f\"  Null count: {episode3_df[col].isnull().sum()}\")\n",
    "    print(f\"  Sample values: {episode3_df[col].head(10).tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RUN COLUMN ANALYSIS:\")\n",
    "print(\"=\"*40)\n",
    "if 'run' in episode3_df.columns:\n",
    "    print(f\"Run column unique values: {episode3_df['run'].unique()}\")\n",
    "    print(f\"Run value counts:\\n{episode3_df['run'].value_counts()}\")\n",
    "else:\n",
    "    print(\"No 'run' column found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"DATA QUALITY CHECKS:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Any completely empty rows: {episode3_df.isnull().all(axis=1).sum()}\")\n",
    "print(f\"Any duplicate rows: {episode3_df.duplicated().sum()}\")\n",
    "\n",
    "# Check for problematic characters or mixed types\n",
    "print(f\"\\nChecking for mixed data types in emotion columns:\")\n",
    "for col in val_cols + aro_cols:\n",
    "    try:\n",
    "        numeric_version = pd.to_numeric(episode3_df[col], errors='coerce')\n",
    "        nan_count = numeric_version.isnull().sum()\n",
    "        original_nan_count = episode3_df[col].isnull().sum()\n",
    "        conversion_issues = nan_count - original_nan_count\n",
    "        if conversion_issues > 0:\n",
    "            print(f\"  {col}: {conversion_issues} values couldn't convert to numeric\")\n",
    "            # Show the problematic values\n",
    "            mask = pd.to_numeric(episode3_df[col], errors='coerce').isnull() & episode3_df[col].notnull()\n",
    "            problematic_values = episode3_df.loc[mask, col].unique()\n",
    "            print(f\"    Problematic values: {problematic_values}\")\n",
    "        else:\n",
    "            print(f\"  {col}: ✅ All values convert to numeric properly\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {col}: ❌ Error checking numeric conversion: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
